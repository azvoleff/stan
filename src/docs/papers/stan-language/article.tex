\documentclass[article]{jss}

\renewcommand{\textfraction}{0.02}
\renewcommand{\topfraction}{0.95}	% max fraction of floats at top





\newenvironment{absolutelynopagebreak}
  {\par\nobreak\vfil\penalty0\vfilneg
   \vtop\bgroup}
  {\par\xdef\tpd{\the\prevdepth}\egroup
   \prevdepth=\tpd}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{{\bf\large Bob Carpenter}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Daniel Lee}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Marcus A.\ Brubaker}
        \\ TTI-Chicago
        %
        \\[9pt]
        %
        {\bf\large Allen Riddell}
        \\ Dartmouth College
    \And
        %
        {\bf\large Andrew Gelman}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Ben Goodrich}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Jiqiang Guo}
        \\ Columbia Univesity
        %
     \And
        %
        {\bf\large Matt Hoffman}
        \\ Adobe Research Labs
        %
        \\[9pt]
        %
        {\bf\large Michael Betancourt}
        \\ University College London
        %
        \\[9pt]
        %
        {\bf\large Peter Li}
        \\ Columbia University
        %
        \\[9pt]
}
\title{\proglang{Stan}: A Probabilistic Programming Language}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Bob Carpenter, Andrew Gelman, Matt Hoffman, 
  Daniel Lee, Ben Goodrich, Michael Betancourt, 
  Marcus Brubaker, Jiqiang Guo, Peter Li} %% comma-separated
\Plaintitle{Stan: A Probabilistic Programming Language} %% without formatting
\Shorttitle{Stan: A Probabilistic Programming Language} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{ \proglang{Stan} is a probabilistic programming language for
  specifying statistical models. A \proglang{Stan} program
  imperatively defines a log probability function over parameters
  conditioned on specified data and constants.  As of version 2.2.0,
  \proglang{Stan} provides full Bayesian inference for continuous-variable
  models through Markov chain Monte Carlo methods such as
  the No-U-Turn sampler, an adaptive form of Hamiltonian Monte
  Carlo sampling.  Penalized maximum likelihood estimates are
  calculated using optimization methods such as the
  Broyden-Fletcher-Goldfarb-Shanno algorithm.

  % This may be contrasted
  % with the \proglang{BUGS} language, in which a program
  % declaratively
  % defines a directed acyclic graphical model.

  % Variables are declared by dimensionality and as to whether they
  % represent data, transformed data, parameters, transformed
  % parameters, or generated quantities; local variables are also
  % supported.  In addition to unconstrained and constrained scalar,
  % discrete, vector, matrix, and array types, there is a library of
  % mathematical functions, probability-related functions, and matrix
  % and linear algebra functions.  Statements, such as assignment,
  % sampling, conditionals, and loops are executed imperatively in the
  % order they are written.

  % \proglang{Stan} programs are translated to \proglang{C++} and
  % compiled.  The target \proglang{C++} code includes forward- and
  % reverse-mode algorithmic differentiation to support samplers and
  % optimizers requiring gradients, Hessians, Hessian-vector products,
  % and other higher-order derivatives.

  % There is an extensive I/O
  % library to deal with constrained variable input for data,
  % initialization for parameters, and output for samples or point
  % estimates.  There are also a range of tools to monitor convergence
  % and mixing, summarize the posterior, perform Bayesian inference,
  % and
  % carry out posterior predictive checks.

  \proglang{Stan} is also a platform for computing log densities and
  their gradients and Hessians, which can be used in alternative
  algorithms such as variational Bayes, expectation propagation, and
  marginal inference using approximate integration.  To this end, \proglang{Stan}
  is set up so that the densities, gradients, and Hessians, along with
  intermediate quantities of the algorithm such as acceptance
  probabilities, are easily accessible.

  \proglang{Stan} can be called from the command line, through
  \proglang{R} using the \pkg{RStan} package, or through
  \proglang{Python} using the \pkg{PyStan} package.  All three
  interfaces support sampling or optimization-based inference and
  analysis, and \pkg{RStan} and \pkg{PyStan} also provide access to
  log probabilities, gradients, Hessians, and data I/O.
}

\Keywords{
  probabilistic program,
  Bayesian inference,
  algorithmic differentiation,
  \proglang{Stan}}
%
\Plainkeywords{
  probabilistic programming,
  Bayesian inference,
  algorithmic differentiation,
  Stan}


%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Bob Carpenter
  \\
  Department of Statistics
  \\ 
  Columbia University
  \\ 
  1255 Amsterdam Avenue
  \\ 
  New York, NY 10027
  \\
  U.S.A.
  \\
  E-mail: \email{carp@alias-i.com}
  \\ 
  URL: \url{http://mc-stan.org/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[Introduction]{Introduction}

The goal of the \proglang{Stan} project is to provide a flexible
probabilistic programming language for statistical modeling along with
a suite of inference tools for fitting models that are robust,
scalable, and efficient.

\proglang{Stan} differs from \proglang{BUGS}
\citep{LunnEtAl:2000,LunnEtAl:2009,LunnEtAl:2012} and
\proglang{JAGS}~\citep{Plummer:2003} in two primary ways.  First,
\proglang{Stan} is based on a new imperative probabilistic programming
language that is more flexible and expressive than the declarative
graphical modeling languages underlying \proglang{BUGS} or
\proglang{JAGS}, in ways such as declaring variables with types and
supporting local variables and conditional statements.  Second,
\proglang{Stan}'s Markov chain Monte Carlo (MCMC) techniques are based
on Hamiltonian Monte Carlo (HMC), a more efficient and robust sampler
than Gibbs sampling or Metropolis-Hastings for models with complex
posteriors.%
%
\footnote{\cite{Neal:2011} analyzes the scaling benfit of HMC with
  dimensionality.  \cite{Hoffman-Gelman:2014} provide practical
  comparisions of \proglang{Stan}'s adaptive HMC algorithm with Gibbs,
  Metropolis, and standard HMC samplers.}

\proglang{Stan} has interfaces for the command-line shell
(\pkg{CmdStan}), \proglang{Python} (\pkg{PyStan}), and \proglang{R}
(\pkg{RStan}), and runs on Windows, Mac OS X, and Linux, and is
open-source licensed.

The next section provides an overview of how \proglang{Stan} works by
way of an extended example, after which the details of \proglang{Stan}'s
programming language and inference mechanisms are provided.



\section{Core Functionality}\label{example.section}

This section describes the use of \proglang{Stan} from the command
line for estimating a Bayesian model using both MCMC sampling for full
Bayesian inference and optimization to provide a point estimate at the
posterior mode.

\subsection{Model for estimating a Bernoulli parameter}

Consider estimating the chance of success parameter for a Bernoulli
distribution based on a sequence of observed binary outcomes.  
Figure~\ref{bernoulli-model.fig} provides an implementation of such a
model in \proglang{Stan}.%
%
\footnote{This model is available in the \proglang{Stan} source
  distribution in \nolinkurl{src/models/basic\_estimators/bernoulli.stan}.}
%
\begin{figure}
\begin{Code}
data {
  int<lower=0> N;                  // N >= 0
  int<lower=0,upper=1> y[N];       // y[n] in { 0, 1 }
}
parameters {
  real<lower=0,upper=1> theta;     // theta in [0, 1]
}
model {
  theta ~ beta(1,1);               // prior
  y ~ bernoulli(theta);            // likelihood
}
\end{Code}
\caption{\it Model for estimating a Bernoulli parameter.}\label{bernoulli-model.fig}
\end{figure}
%
The model treats the observed binary data, \code{y[1],...,y[N]}, as
independent and identically distributed, with success probability
\code{theta}.  The vectorized likelihood statement can also be coded
using a loop as in \proglang{BUGS}, although it will run more slowly than the
vectorized form:
%
\begin{absolutelynopagebreak}
\begin{Code}
for (n in 1:N)
  y[n] ~ bernoulli(theta);
\end{Code}
\end{absolutelynopagebreak}
%
A \code{beta(1,1)} (i.e., uniform) prior is placed on \code{theta},
although there is no special behavior for conjugate priors in
\proglang{Stan}.  The prior could be dropped from the model altogether
because parameters start with uniform distributions on their support,
here constrained to be between 0 and 1 in the parameter declaration
for \code{theta}.  



\subsection{Data format}

Data for running \proglang{Stan} from the command line can be included
in \proglang{R} dump format.  All of the variables declared in the
data block of the \proglang{Stan} program must be defined in the data
file.  For example, 10 observations for the model in
Figure~\ref{bernoulli-model.fig} could be encoded as%
%
\footnote{This data
file is provided with the \proglang{Stan} distrbution in file
\nolinkurl{src/models/basic\_estimators/bernoulli.R.stan}.}
%
\begin{absolutelynopagebreak}
\begin{Code}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Code}
\end{absolutelynopagebreak}
%
This defines the contents of two variables, an integer \code{N} and a
10-element integer array \code{y}.  The variable \code{N} is declared
in the data block of the program as being an integer greater than or
equal to zero;  the variable \code{y} is declared as an integer array of size
\code{N} with entries between 0 and 1 inclusive.

In \pkg{RStan} and \pkg{PyStan}, data can also be passed directly
through memory without the need to read or write to a file.  

\subsection{Compling the model}

After a \proglang{C++} compiler and \code{make} are installed,%
%
\footnote{Appropriate versions are built into Linux. The \pkg{RTools}
  package suffices for Windows; it is available from
  \url{http://cran.r-project.org/bin/windows/Rtools/}.  The
  \pkg{Xcode} package contains everything needed for the Mac; see
  \url{https://developer.apple.com/xcode/} for more information.}
%
the Bernoulli model in Figure~\ref{bernoulli-model.fig} can be
translated to \proglang{C++} and compiled with a single command.
First, the directory must be changed to \code{$stan}, which we use as
a shorthand for the directory in which \proglang{Stan} was unpacked.%
%
\footnote{Before the first model is built, \code{make} must build the
  model translator (target \code{bin/stanc}) and posterior summary tool
  (target \code{bin/print}), along with an optimized version of the
  \proglang{C++} library (target \code{bin/libstan.a}).  Please be patient
  and consider \code{make} option \code{-j2} or \code{-j4} (or higher)
  to run in the specified number of processes if two or four (or more)
  computational cores are available.}
%
\begin{CodeChunk}
\begin{CodeInput}
> cd $stan
> make src/models/basic_estimators/bernoulli 
\end{CodeInput}
\end{CodeChunk}
%
This produces an executable file \code{bernoulli}
(\code{bernoulli.exe} on Windows) on the same path as the model.
Forward slashes can be used with \code{make} on Windows.

\subsection{Running the sampler}

\subsubsection{Command to sample from the model}

The executable can be run with default options by specifying a path to
the data file.  The first command in the following example changes the
current directory to that containing the model, which is where the
data resides and where the executable is built.  From there, the path
to the data is just the file name \code{bernoulli.data.R}.
%
\begin{CodeChunk}
\begin{CodeInput}
> cd $stan/src/models/basic_estimators
> ./bernoulli sample data file=bernoulli.data.R
\end{CodeInput}
\end{CodeChunk}
%
For Windows, the \code{./} before the command should be removed.  This
call specifies that sampling should be performed with the model
instantiated using the data in the specified file.

\subsubsection{Terminal output from sampler}

The output is as follows, starting with a summary of the command-line
options used, including defaults;  these are also written into the 
samples file as comments.
%
\begin{Code}
 method = sample (Default)
   sample
     num_samples = 1000 (Default)
     num_warmup = 1000 (Default)
     save_warmup = 0 (Default)
     thin = 1 (Default)
     adapt
       engaged = 1 (Default)
       gamma = 0.050000000000000003 (Default)
       delta = 0.80000000000000004 (Default)
       kappa = 0.75 (Default)
       t0 = 10 (Default)
       init_buffer = 75 (Default)
       term_buffer = 50 (Default)
       window = 25 (Default)
     algorithm = hmc (Default)
       hmc
         engine = nuts (Default)
           nuts
             max_depth = 10 (Default)
         metric = diag_e (Default)
         stepsize = 1 (Default)
         stepsize_jitter = 0 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 4294967295 (Default)
 output
   file = output.csv (Default)
   diagnostic_file =  (Default)
   refresh = 100 (Default)

Gradient evaluation took 4e-06 seconds
1000 transitions using 10 leapfrog steps per transition would take
0.04 seconds.
Adjust your expectations accordingly!

Iteration:    1 / 2000 [  0%]  (Warmup)
Iteration:  100 / 2000 [  5%]  (Warmup)
...
Iteration: 1000 / 2000 [ 50%]  (Warmup)
Iteration: 1001 / 2000 [ 50%]  (Sampling)
...
Iteration: 2000 / 2000 [100%]  (Sampling)

  Elapsed Time: 0.00932 seconds (Warm-up)
                0.016889 seconds (Sampling)
                0.026209 seconds (Total)
\end{Code}
%
The sampler configuration parameters are echoed, here they are all
default values other than the data file.  

The command-line parameters marked \code{Default} may be explicitly
set on the command line.  Each value is preceded by the full path to
it in the hierarchy; for instance, to set the maximum depth for the
no-U-turn sampler, the command would be the following, where backslash
indicates a continued line.
%
\begin{CodeChunk}
\begin{CodeInput}
> ./bernoulli sample  \
  algorithm=hmc engine=nuts max_depth=5  \
  data max_depthfile=bernoulli.data.R
\end{CodeInput}
\end{CodeChunk}
%


\subsubsection{Help}

A description of all configuration parameters including default values
and constraints is available by executing
%
\begin{CodeChunk}
\begin{CodeInput}
> ./bernoulli help-all
\end{CodeInput}
\end{CodeChunk}
%
The sampler and its configuration are described at greater length in
the manual \citep{Stan:2013}.

\subsubsection{Samples file output}

The output CSV file, written by default to \code{output.csv}, starts
with a summary of the configuration parameters for the run.
%
\begin{Code}
# stan_version_major = 2
# stan_version_minor = 1
# stan_version_patch = 0
# model = bernoulli_model
# method = sample (Default)
#   sample
#     num_samples = 1000 (Default)
#     num_warmup = 1000 (Default)
#     save_warmup = 0 (Default)
#     thin = 1 (Default)
#     adapt
#       engaged = 1 (Default)
#       gamma = 0.050000000000000003 (Default)
#       delta = 0.80000000000000004 (Default)
#       kappa = 0.75 (Default)
#       t0 = 10 (Default)
#       init_buffer = 75 (Default)
#       term_buffer = 50 (Default)
#       window = 25 (Default)
#     algorithm = hmc (Default)
#       hmc
#         engine = nuts (Default)
#           nuts
#             max_depth = 10 (Default)
#         metric = diag_e (Default)
#         stepsize = 1 (Default)
#         stepsize_jitter = 0 (Default)
# id = 0 (Default)
# data
#   file = bernoulli.data.R
# init = 2 (Default)
# random
#   seed = 847896134
# output
#   file = output.csv (Default)
#   diagnostic_file =  (Default)
#   refresh = 100 (Default)
\end{Code}
%
\proglang{Stan}'s behavior is fully specified by these configuration
parameters, almost all of which have default values.  The
\code{sample} configuration

 By using the same version of
\proglang{Stan} and these configuration parameters, exactly the same
output file can be reproduced.  The pseudorandom numbers generated by
the sampler are fully determined by the seed (here randomly generated
based on the time of the run, with value \code{847896134}) and the
identifier (here \code{0}).  The identifier is used to advance the
underlying pseudorandom number generator a sufficient number of values
that using multiple chains with the same seed and different
identifiers will draw from different subsequences of the pseudorandom
number stream determined by the seed.

The output contiues with a CSV header naming the columns of the
output. For the default NUTS sampler in \proglang{Stan} 2.2.0, these are
%
\begin{Code}
lp__,accept_stat__,stepsize__,treedepth__,n_divergent__,theta
\end{Code}
%
The values headed by \code{lp\_\_} are the log densities (up to an
additive constant), \code{accept\_stat\_\_} are the Metropolis
acceptance proababilities averaged over samples in the slice used by
the no-U-turn sampler, \code{stepsize\_\_} is the leapfrog
integrator's step size for simulating the Hamiltonian,
\code{treedepth\_\_} is the depth of tree explored by the no-U-turn
sampler, and \code{n\_divergent\_\_} is the number of iterations leading
to a numerical instability during integration (e.g., numerical
overflow or a positive-definiteness violation).

 for each iteration.%
%
\footnote{Acceptance is the usual notion for a Metropolis sampler such
  as HMC \citep{MetropolisEtAl:1953}.  For NUTS, the acceptance
  statistic is defined as the average acceptance probabilities of all
  possible samples in the proposed tree; NUTS itself uses a slice
  sampling algorithm for rejection \citep{Neal:2003, HoffmanGelman:2011}.}
%
The column \code{stepsize\_\_} indicates the step size
(i.e., time interval) of the simulated trajectory, while the column 
\code{treedepth\_\_} gives the tree depth for NUTS, defined as the 
log base 2 of the total number of steps in the trajectory.
The rest of the header will be the names of parameters; in this
example, \code{theta} is the only parameter.

Next, the results of adaptation are printed as comments.
%
\begin{Code}
# Adaptation terminated
# Step size = 0.783667
# Diagonal elements of inverse mass matrix:
# 0.517727
\end{Code}
%
By default, \proglang{Stan} uses the NUTS sampler with a diagonal mass
matrix.  The mass matrix is estimated, roughly speaking, by
regularizing the sample covariance of the latter half of the warmup
samples; see \citep{Stan:2013} for full details.  A dense mass matrix
may also be estimated, or the mass matrix may be set to the unit matrix.

The rest of the file contains samples, one per line, matching the
header; here the parameter \code{theta} is the final value printed on
each line, and each line corresponds to a sample.  The warmup samples
are not included by default, but may be included with the appropriate
command-line invocation of the executable. The file ends with comments reporting the elapsed time.
%
\begin{Code}
-7.19297,1,0.783667,1,0,0.145989
-8.2236,0.927238,0.783667,1,0,0.0838792
...
-7.48489,0.738509,0.783667,0,0,0.121812
-7.40361,0.995299,0.783667,1,0,0.407478
-9.49745,0.771026,0.783667,2,0,0.0490488
-9.11119,1,0.783667,0,0,0.0572588
-7.20021,0.979883,0.783667,1,0,0.14527

#  Elapsed Time: 0.010849 seconds (Warm-up)
#                0.01873 seconds (Sampling)
#                0.029579 seconds (Total)
\end{Code}
%
It is evident from the values sampled for \code{theta} in the last
column that there is a high degree of posterior uncertainty in the
estimate of \code{theta} from the ten data points in the data file.

The log probabilities reported in the first column include not only
the model log probabilities but also the Jacobian adjustment resulting
from the transformation of the variables to unconstrained space.
Here, that is the absolute derivative of the inverse logistic
function;  see \citep{Stan:2013} for full details on all of the
transforms and their Jacobians.


\subsection{Sampler output analysis}

Before performing output analysis, we recommend generating multiple
independent chains in order to more effectively monitor convergence;
see \citep{GelmanRubin:1992} for more analysis.  Three more chains of
samples can be created as follows.
%
\begin{CodeChunk}
\begin{CodeInput}
./bernoulli sample data file=bernoulli.data.R random seed=847896134 \
            id=1 output file=output1.csv
./bernoulli sample data file=bernoulli.data.R random seed=847896134 \
            id=2 output file=output2.csv
./bernoulli sample data file=bernoulli.data.R random seed=847896134 \
            id=3 output file=output3.csv
\end{CodeInput}
\end{CodeChunk}
%
These calls illustrate how additional parameters are specified
directly on the command line following the hierarchy given in the
output.  The backslash (\code{\textbackslash}) at the end of each line indicates
that the command continues on the last line;  a caret (\code{\textasciicircum})
should be used in Windows.

The chains can be safely run in parallel under different processes;
details of parallel execution depend on the operating system and the shell or
terminal program. Note that, although the same seed is used for each chain, the
random numbers will in fact be independent as the chain identifier
is used to skip the pseudorandom number generator ahead.  See
Section~\ref{rng.section} for more information.

\begin{figure}
\begin{Code}
Inference for Stan model: bernoulli_model
4 chains: each with iter=(1000,1000,1000,1000); warmup=(0,0,0,0); 
                    thin=(1,1,1,1); 4000 iterations saved.

Warmup took (0.0108, 0.0130, 0.0110, 0.0110) seconds, 0.0459 seconds total
Sampling took (0.0187, 0.0190, 0.0168, 0.0176) seconds, 0.0722 seconds total

                 Mean      MCSE  StdDev         5%    50%    95%
lp__            -7.28  1.98e-02   0.742  -8.85e+00  -6.99  -6.75
accept_stat__   0.909  4.98e-03   0.148   5.70e-01  0.971   1.00
stepsize__      0.927  7.45e-02   0.105   7.84e-01   1.00   1.05
treedepth__     0.437  1.03e-02   0.551   0.00e+00  0.000   1.00
n_divergent__   0.000  0.00e+00   0.000   0.00e+00  0.000  0.000
theta           0.254  3.25e-03   0.122   7.58e-02  0.238  0.479

                N_Eff  N_Eff/s     R_hat
lp__             1404    19447  1.00e+00
accept_stat__     887    12297  1.02e+00
stepsize__       2.00     27.7  5.56e+13
treedepth__      2856    39572  1.01e+00
n_divergent__    4000    55424       nan
theta            1399    19382  1.00e+00
\end{Code}
\caption{\it Output of \code{bin/print} for the Bernoulli estimation model in
  Figure~\ref{bernoulli-model.fig}.}\label{print-output.fig}
\end{figure}

\proglang{Stan} supplies a command-line program \code{bin/print} to
summarize the output of one or more MCMC chains.  Given a directory
containing output from sampling,
%
\begin{CodeChunk}
\begin{CodeInput}
> ls output*.csv
\end{CodeInput}
\begin{CodeOutput}
output.csv	output1.csv	output2.csv	output3.csv
\end{CodeOutput}
\end{CodeChunk}
%
posterior summaries are printed using
%
\begin{CodeChunk}
\begin{CodeInput}
> $stan/bin/print output*.csv
\end{CodeInput}
\end{CodeChunk}
%
The output is shown in Figure~\ref{print-output.fig}.%
%
\footnote{Aligning columns when printing rows of varying scales
  presents a challenge.  For each column, the program calculates the
  the maximum number of digits required to print an entry in that
  column with the specified precision. For example, a precision of 2
  for the number -0.000012 requires nine characters (\code{-0.000012})
  to print without scientific notation versus seven digits with
  (\code{-1.2e-5}).  If the discrepancy is above a fixed threshold,
  scientific notation is used.  Compare the results in the \code{mean}
  column versus the \code{sd} column.}
%
Each row of the output summarizes a different value whose name is
provided in the first column.  These correspond to the columns in the
output CSV files. The analysis includes estimates of the posterior
mean (\code{Mean}) and standard deviation (\code{StdDev}).  The median
(\code{50\%}) and 90\% posterior interval (\code{5\%}, \code{95\%})
are also displayed.  

The remaining columns in the output provide an analysis of the
sampling and its efficiency.  The convergence diagnostic that is built
into the \code{bin/print} command is the estimated potential scale
reduction statistic $\hat{R}$ (\code{Rhat}); its value should be close
to 1.0 when the chains have all converged to the same stationary
distribution.  \proglang{Stan} uses a more conservative version of
$\hat{R}$ than is usual in packages such as \pkg{Coda}
\citep{PlummerEtAl:2006}, first splitting each chain in half to
diagnose nonstationary chains; see \citep{GelmanEtAl:2013} and
\citep{Stan:2013} for detailed definitions.

The column \code{N\_eff} is the number of effective samples in a
chain.  Because MCMC methods produce correlated samples in each chain,
estimates such as posterior means are not as accurate as they would be
with truly independent samples.  The number of effective samples is an
estimate of the number of independent samples that would lead to the
same accuracy.  The Monte Carlo standard error (\code{MCSE}) is an
estimate of the error in estimating the posterior mean based on
dividing the posterior standard deviation estimate by the square root
of the number of effective samples (\code{sd / sqrt(n\_eff)}).
\cite{Geyer:2011} provides a thorough introduction to effective sample
size and MCSE estimation.  \proglang{Stan} uses the more conservative
estimates based on both within-chain and cross-chain convergence; see
\citep{GelmanEtAl:2013} and \citep{Stan:2013} for motivation and
definitions.

Because estimation accuracy is governed by the square root of the
number of effective samples, effective samples per second (or seconds
per effective sample) is the most relevant statistic for comparing the
efficiency of sampler implementations.  Compared to \proglang{BUGS}
and \proglang{JAGS}, \proglang{Stan} is often relatively slow per
iteration but relatively fast per effective sample.

In this example, the estimated number of effective samples per
parameter (\code{n\_eff}) is 1399, which far more than we typically
need for inference.  The posterior mean here is estimated to be 0.254
with an MCSE of 0.00325.  Because the model is conjugate, the exact
posterior is known to be $p(\theta|y) = \mbox{\sf Beta}(3,9)$.  Thus
the posterior mean of $\theta$ is $3/(3+9) = 0.25$ and the posterior
mode of $\theta$ is $(3-1)/(3 + 9 - 2) = 0.2$.


\subsection{Posterior mode estimates}

\subsubsection{Posterior modes with optimization}

The posterior mode of a model can be found by using one of
\proglang{Stan}'s built-in optimizers.  The following command invokes
optimization for the Bernoulli model using all default configuration
parameters.
%
\begin{CodeChunk}
\begin{CodeInput}
> ./bernoulli optimize data file=bernoulli.data.R 
\end{CodeInput}
\begin{CodeOutput}
 method = optimize
   optimize
     algorithm = bfgs (Default)
       bfgs
         init_alpha = 0.001 (Default)
         tol_obj = 1e-08 (Default)
         tol_grad = 1e-08 (Default)
         tol_param = 1e-08 (Default)
     iter = 2000 (Default)
     save_iterations = 0 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 4294967295 (Default)
 output
   file = output.csv (Default)
   diagnostic_file =  (Default)
   refresh = 100 (Default)

initial log joint probability = -12.4873
    Iter      log prob        ||dx||      ||grad||   alpha  # evals  Notes 
       7      -5.00402   8.61455e-07   1.25715e-10       1       10   
Optimization terminated normally: 
  Convergence detected: change in objective function was below
  tolerance
\end{CodeOutput}
\end{CodeChunk}
%
The final lines of the output indicate normal termination after seven
iterations by convergence of the objective function (here the log
probability) to the default tolerance of \code{1e-08}. The final log
probability (\code{log prob}), length of the difference between the
current iteration's value of the parameter vector and the previous
value (\code{||dx||}), and the length of the gradient vector
(\code{||grad||}).

The optimizer terminates when any of the log probability, gradient, or
parameter values are within their specified tolerance.  The default
optimizer uses the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm,
a quasi-Newton method which employs exactly computed gradients and an
efficient approximation to the Hessian; see \citep{NocedalWright:2006}
for a textbook exposition of the BFGS algorithm.

\subsubsection{Optimizer output file}

By default, optimizations results are written into \code{output.csv},
which is a valid CSV file.
%
\begin{CodeChunk}
\begin{CodeOutput}
# stan_version_major = 2
# stan_version_minor = 1
# stan_version_patch = 0
# model = bernoulli_model
# method = optimize
#   optimize
#     algorithm = bfgs (Default)
#       bfgs
#         init_alpha = 0.001 (Default)
#         tol_obj = 1e-08 (Default)
#         tol_grad = 1e-08 (Default)
#         tol_param = 1e-08 (Default)
#     iter = 2000 (Default)
#     save_iterations = 0 (Default)
# id = 0 (Default)
# data
#   file = bernoulli.data.R
# init = 2 (Default)
# random
#   seed = 777510854
# output
#   file = output.csv (Default)
#   diagnostic_file =  (Default)
#   refresh = 100 (Default)
lp__,theta
-5.00402,0.2000000000125715
\end{CodeOutput}
\end{CodeChunk}
%
As with the sampler output, the configuration of the optimizer is
dumped as CSV comments (lines beginning with \code{\#}).  Then there
is a header, listing the log probability, \code{lp\_\_}, and the
single parameter name, \code{theta}.  The next line shows that the
posterior mode for \code{theta} is 0.2000000000125715, matching the
true posterior mode of 0.20 very closely.

Optimization is carried out on the unconstrained parameter space, but
without the Jacobian adjustment to the log probability.  This ensures
modes are defined with respect to the constrained parameter space as
declared in the parameters block and used in the model specification.
The need to suppress the Jacobian to match the scaling of the declared
parameters highlights the sensitivity of posterior modes to parameter
transforms. 

\subsection{Diagnostic mode}

\proglang{Stan} provides a diagnostic mode that evaluates the log
probability and gradient calculations at the initial parameter values
(either user supplied or generated randomly based on the specified
or default seed).
%
\begin{CodeChunk}
\begin{CodeInput}
> ./bernoulli diagnose data file=bernoulli.data.R 
\end{CodeInput}
\begin{CodeOutput}
 method = diagnose
   diagnose
     test = gradient (Default)
       gradient
         epsilon = 9.9999999999999995e-07 (Default)
         error = 9.9999999999999995e-07 (Default)
 id = 0 (Default)
 data
   file = bernoulli.data.R
 init = 2 (Default)
 random
   seed = 4294967295 (Default)
 output
   file = output.csv (Default)
   diagnostic_file =  (Default)
   refresh = 100 (Default)

TEST GRADIENT MODE
 Log probability=-6.74818
 param idx           value           model     finite diff           error
         0         -1.1103       0.0262302       0.0262302    -3.81445e-10
\end{CodeOutput}
\end{CodeChunk}
%
Here, a random initialization is used and the initial log probability
is \code{-6.74818} and the single parameter \code{theta}, here
represented by index 0, has a value of \code{-1.1103} on the
unconstrained scale.  The derivative supplied by the model and by a
finite differences calculation are the same to within
\code{-3.81445e-10}.  Non-finite log probability values or derivatives
indicate a problem with the model in terms of constraints on parameter
values or function inputs being violated, boundary conditions in
functions, and sometimes overflow or underflow issues with
floating-point calculations.  Errors between the model's gradient
calculation and finite differences can indicate a bug in
\proglang{Stan}'s algorithmic differentiation for a function in the model.

\subsection{Roadmap for the Rest of the Paper}

Now that the key functionality of \proglang{Stan} has been
demonstrated, the remaining sections cover specific aspects of
\proglang{Stan}'s architecture.  Section~\ref{data-types.section}
covers variable data type declarations as well as expressions and type
inference, Section~\ref{programming.section} describes the top-level
blocks and execution of a \proglang{Stan} program,
Section~\ref{statements.section} lays out the available statements,
and Section~\ref{functions.section} the built-in math, matrix, and
probability function library.  Section~\ref{inference-engines.section}
lays out MCMC and optimization-based inference.  There are two
appendices, Appedix~\ref{process.section} outlining the development
process and Appendix~\ref{libraries.section} detailing the library
dependencies.


\section{Data types}\label{data-types.section}

All expressions in \proglang{Stan} are statically typed, including
variables.  This means their type is declared at compile time as part
of the model, and does not change throughout the execution of the
program.  This is the same behavior as is found in compiled
programming languages such as \proglang{C(++)}, \proglang{Fortran},
and \proglang{Java}, but is unlike the behavior of interpreted
languages such as \proglang{BUGS}, \proglang{R}, and
\proglang{Python}.  Statically typing the variables (as well as
declaring them in appropriate blocks based on usage) makes
\proglang{Stan} programs easier to read and easier to debug by making
explicit the modeling decisions and expression types.

\subsection{Primitive types}

The primitive types of \proglang{Stan} are \code{real} and \code{int},
which are used to represent continuous and integer values.  These
values are represented directly in \proglang{C++} as types
\code{double} and \code{int}.  Integer expressions can be used
anywhere a real value is required, but not {\it vice-versa}.  

\subsection{Vector and matrix types}

\proglang{Stan} supports vectors, row vectors, and matrices with the
usual access operations.  Indexing for vector, matrix, and array types
starts from one.  

Vectors are declared with their sizes and matrices with their number
of rows and columns.  Vector, row vector, and matrix elements are
accessed using bracket notation, as in \code{y[3]} for the third
element of a vector or row vector and \code{a[2,3]} for the element in
the third column of the second row of a matrix.  Indexing begins from
1.  The notation \code{a[2]} accesses the second row of matrix \code{a}.

All vector and matrix types contain real values and may not be
declared to contain integers.  Collections of integers are represented
using arrays.



\subsection{Array types}

An array may have entries of any other type.  For example, arrays of
integers and reals are allowed, as are arrays of vectors or arrays of
matrices.  

Higher-dimensional arrays are intrinsically arrays of arrays.  An
entry in a two-dimensional array \code{y} may be accessed as
\code{y[1,2]}.  The expression \code{y[1]} by itself denotes the
one-dimensional array whose values correspond to the first row of
\code{y}.  Thus \code{y[1][2]} has the same value as \code{y[1,2]}.%
%
\footnote{Arrays are stored in row-major order and matrices in
  column-major order.}

Unlike integers, which may be used where real values are required,
arrays of integers may not be used where real arrays are required.%
%
\footnote{In the language of type theory, \proglang{Stan} arrays are
  not covariant.  This follows the behavior of both arrays and
  standard library containers in \proglang{C++}.}

The manual contains a chapter discussing the efficiency tradeoffs and
motivations for separating arrays and matrices.

\subsection{Constrained variable types}

Variables may be declared with constraints.  The constraints have
different effects depending on the block in which the variable is
declared.

Integer and real types may be provided with lower bounds, upper
bounds, or both.  This includes the types used in arrays, and the real
types used in vectors and matrices.

Vector types may be constrained to be unit simplexes (all entries
non-negative and summing to 1), unit length vectors (sum of squares is 1), or
ordered (entries are in ascending order), positive ordered (entries in
ascending order, all non-negative), using the types \code{simplex[K]},
\code{unit_vector[K]}, \code{ordered[K]}, or
\code{positive_ordered[K]}, where \code{K} is the size of the vector.

Matrices may be constrained to be covariance matrices (symmetric,
positive definite) or correlation matrices (symmetric, positive
definite, unit diagonal), using the types \code{cov_matrix[K]} and
\code{corr_matrix[K]}.


\subsection{Expressions}\label{expressions.section}

The syntax of \code{Stan} is defined in terms of expressions and
statements.  Expressions denote values of a particular type.  
Statements represent operations such as assignment and sampling as
well as control structures such as for loops and conditionals.

\proglang{Stan} provides the usual kinds of expressions found in
programming languages.  This includes variables, literals denoting
integers, real values or strings, binary and unary operators over
expressions, and function application.  

\subsubsection{Type inference}

The type of a numeric literal is determined by whether or not it
contains a period or scientific notation; for example, \code{20} has
type \code{int} whereas \code{20.0} and \code{2e+1} have type
\code{real}.

The type of applying an operator or a function to one or more
expressions is determined by the available signatures for the
function.  For example, the multiplication operator (\code{*}) has a
signature that maps two \code{int} arguments to an \code{int} and two
\code{real} arguments to a \code{real} result.  Another signature for
the same operator maps a \code{row\_vector} and a \code{vector} to a
\code{real} result.

\subsubsection{Type promotion}

If necessary, an integer type will be promoted to a \code{real} value.
For example, multiplying an \code{int} by a \code{real} produces a
\code{real} result by promoting the \code{int} argument to a
\code{real}.  



\section{Top-Level Blocks and Program Execution}\label{programming.section}\label{hierarchical-modeling.section}

In the rest of this paper, we will concentrate on the modeling
language and how compiled models are executed.  These details are the
same whether a \proglang{Stan} model is being used by one of the
built-in samplers or optimizers or being used externally by a
user-defined sampler or optimizer.   

We begin with an example that will be used throughout the rest of this
section.  \cite[Section 5.1]{GelmanEtAl:2013} define a hierarchical
model of the incidence of tumors in rats in control groups across
trials; a very similiar model is defined for mortality rates in
pediatric surgeries across hospitals in \citep[Examples, Volume
1]{LunnEtAl:2000,LunnEtAl:2009}.
%  
\begin{figure}
\begin{Code}
data {
  int<lower=0> J;                         // number of items
  int<lower=0> y[J];                      // number of successes for j
  int<lower=0> n[J];                      // number of trials for j
}
parameters {
  real<lower=0,upper=1> theta[J];        // chance of success for j
  real<lower=0,upper=1> lambda;          // prior mean chance of success
  real<lower=0.1> kappa;                 // prior count
}
transformed parameters {
  real<lower=0> alpha;                   // prior success count
  real<lower=0> beta;                    // prior failure count
  alpha <- lambda * kappa;
  beta <- (1 - lambda) * kappa;
}
model {
  lambda ~ uniform(0,1);                 // hyperprior
  kappa ~ pareto(0.1,1.5);               // hyperprior 
  theta ~ beta(alpha,beta);              // prior
  y ~ binomial(n,theta);                 // likelihood
}
generated quantities {
  real<lower=0,upper=1> avg;             // avg success
  int<lower=0,upper=1> above_avg[J];     // true if j is above avg
  int<lower=1,upper=J> rnk[J];           // rank of j
  int<lower=0,upper=1> highest[J];          // true if j is highest rank
  avg <- mean(theta);
  for (j in 1:J)
    above_avg[j] <- (theta[j] > avg);
  for (j in 1:J) {
    rnk[j] <- rank(theta,j) + 1;
    highest[j] <- rnk[j] == 1;
  }
}
\end{Code}
  \caption{\it Hierarchical binomial model with posterior inferences,
    coded in \proglang{Stan}.}\label{hier-binom.fig}
\end{figure}
%
A \proglang{Stan} implementation is provided in
Figure~\ref{hier-binom.fig}.  In the rest of this section, we will
walk through what the meaning of the various blocks are for the
execution of the model.

\subsection{Data block}

A \proglang{Stan} program starts with an (optional) data block, which
declares the data required to fit the model.  This is a very different
approach to modeling and declarations than in \proglang{BUGS} and
\proglang{JAGS}, which determine which variables are data and which
are parameters at run time based on the shape of the data input to
them.  These declarations make it possible to compile \proglang{Stan}
to much more efficient code.%
%
\footnote{The speedup is because coding data variables as
  \code{double} types in \proglang{C++} is much faster than promoting
  all values to algorithmic differentiation class variables.}
%
Missing data models may still be coded in \proglang{Stan}, but the
missing values must be declared as parameters; see \citep{Stan:2013}
for examples of missing data, censored data, and truncated data
models.

In the model in Figure~\ref{hier-binom.fig}, the data block declares
an integer variable \code{J} for the number of groups in the
hierarchical model.  The arrays \code{y} and \code{n} have size
\code{J}, with \code{y[j]} being the number of positive outcomes in
\code{n[j]} trials.

All of these variables are declared with a lower-bound constraint
restricting their values to be greater than or equal to zero.
\proglang{Stan}'s constraint language is not strong enough to restrict
each \code{y[j]} to be less than or equal to \code{n[j]}.

The data for a \proglang{Stan} model is read in once as the
\proglang{C++} object representing the model is constructed.  After
the data is read in, the constraints are validated.  If the data does
not satisfy the declared constraints, the model will throw an
exception with an informative error message, which is displayed to the
user in the command-line, R, and Python interfaces.

\subsection{Transformed data block}

The model in Figure~\ref{hier-binom.fig} does not have a transformed
data block.  A transformed data block may be used to define new
variables that can be computed based on the data.  For example,
standardized versions of data can be defined in a transformed
data block or Bernoulli trials can be summed to model as binomial.
Any constant data can also be defined in the transformed data block.

The transformed data block starts with a sequence of variable
declarations and continues with a sequence of statements defining the
variables.  For example, the following transformed data block declares
a vector \code{x\_std}, then defines it to be the standardization of \code{x}:
%
\begin{Code}
transformed data {
  vector[N] x_std;
  x_std <- (x - mean(x)) / sd(x);
}
\end{Code}

The transformed data block is executed during construction, after the
data is read in.  Any data variables declared in the data block may be
used in the variable declarations or statements.  Transformed data
variables may be used after they are declared, although care must be
taken to ensure they are defined before they are used.  Any
constraints declared on transformed data variables are validated after
all of the statements are executed, with execution terminating with an
informative error message at the first variable with an invalid value.

\subsection{Parameter block}

The parameter block in the program in Figure~\ref{hier-binom.fig}
defines three parameters.  The parameter \code{theta[j]} represents
the probability of success in group \code{j}.  The prior on each
\code{theta[j]} is parameterized by a prior mean chance of success
\code{lambda} and prior count \code{kappa}.  Both \code{theta[j]} and
\code{lambda} are constrained to fall between zero and one, whereas
\code{kappa} is constrained to be greater than or equal to 0.1 to
match the support of the Pareto hyperprior it receives in the model block.

The parameter block is executed every time the log probability is
evaluated.  This may be multiple times per iteration of a sampling or
optimization algorithm.  

\subsubsection{Implicit change of variables to unconstrained space}

The probability distribution defined by a \proglang{Stan} program is
intended to have unconstrained support (i.e., no points of zero
probability), which greatly simplifies the task of writing samplers or
optimizers.  To achieve unbounded support, variables declared with
constrained support are transformed to an unconstrained space.  For
instance, variables declared on $[0,1]$ are log-odds transformed and
non-negative variables declared to fall in $[0,\infty)$ are log
transformed.  More complex transforms are required for simplexes (a
reverse stick-breaking transform) and covariance and correlation
matrices (Cholesky factorization).  The dimensionality of the
resulting probability function may change as a result of the
transform. For example, a $K \times K$ covariance matrix requires only
${K \choose 2} + K$ unconstrained parameters, and a $K$-simplex
requires only $K-1$ unconstrained parameters.

The unconstrained parameters over which the model is defined are
inverse transformed back to their constrained forms before executing
the model code.  To account for the change of variables, the log
absolute Jacobian determinant of the inverse transform is added to the
overall log probability function.%
%
\footnote{For optimization, the Jacobian adjustment is suppressed to
  guarantee the optimizer finds the maximum of the log probability
  function on the constrained parameters.  The calculation of the
  Jacobian is controlled by a template parameter in the \proglang{C++}
  code generated for a model.}
%
The gradients of the log probability function exposed include the
Jacobian term.  

There is no validation required for the parameter block because the
variable transforms are guaranteed to produce values that satisfy the
declared constraints.


\subsection{Transformed parameters block}

The transformed parameters block allows users to define transforms of
parameters within a model.  Following the model in
\citep{GelmanEtAl:2013}, the example in Figure~\ref{hier-binom.fig}
uses the transformed parameter block to define transformed parameters
\code{alpha} and \code{beta} for the prior success and failure counts
to use in the beta prior for \code{theta}.  

Following the same convention as the transformed data block, the
(optional) transformed parameter block begins with declarations of the
transformed parameters, followed by a sequence of statements defining
them.  Variables from previous blocks as well as the transformed
parameters block may be used.  In the example, the prior success and
failure counts \code{alpha} and \code{beta} are defined in terms of
the prior mean \code{lambda} and total prior count \code{kappa}.

The transformed parameter block is executed after the parameter block.
Constraints are validated after all of the statements defining the
transformed parameters have executed.  Failure to validate a
constraint results in an exception being thrown, which halts the
execution of the log probability function.  The log probability
function can be defined to return negative infinity or the special
not-a-number value, both of which are available through built-in
functions and may be passed to the \code{increment\_log\_prob}
function (see below).

If transformed parameters are used on the left-hand side of a sampling
statement, it is up to the user to add the appropriate log absolute
Jacobian determinant adjustment to the log probability accumulator.
For instance, a lognormal variate could be generated as follows
without the built-in \code{lognormal} density function using the
normal density as
%
\begin{Code}
parameters {
  real<lower=0> u;
  ...
transformed parameters {
  real v;
  v <- log(u);
  // log absolute Jacobian determinant adjustment
  increment_log_prob(u);      
}
model {
  v ~ normal(0,1);
}
\end{Code}
%
The transorm is $f(u) = \log u$, the inverse transform is $f^{-1}(v) =
\exp v$, so the absolute log Jacobian determinant is $|\frac{d}{dv}
\exp v| = \exp v = u$.  Whenever a transformed parameter is used on
the left side of a sampling statement, a warning is printed to remind
the user of the need for a Jacobian adjustment for the change of
variables.

The \code{increment\_log\_prob} statement is used to add a term to the
total log probability function defined by the model block and the log
absolute Jacobian determinants of the transforms.  The variable
\code{lp\_\_}, representing the currently accumulated total log
density, may not be assigned to directly.


Values of transformed parameters are saved in the output along
with the parameters.  As an alternative, local variables can be used
to define temporary values that do not need to be saved.  

\subsection{Model block}

The purpose of the model block is to define the log probability
function on the constrained parameter space.  The example in
Figure~\ref{hier-binom.fig} has a simple model containing four
sampling statements.  The hyperprior on the prior mean \code{lambda}
is uniform, and the hyperprior on the prior count \code{kappa} is a
Pareto distribution with lower-bound of support at 0.1 and shape 1.5,
leading to a probability of $\kappa > 0.1$ proportional to
$\kappa^{-5/2}$.  Note that the hierarchical prior on \code{theta} is
vectorized: each element of \code{theta} is drawn independently from a
beta distribution with prior success count \code{alpha} and prior
failure count \code{beta}.  Both \code{alpha} and \code{beta} are
transformed parameters, but because they are only used on the
right-hand side of a sampling statement do not require a Jacobian
adjustment of their own.  The likelihood function is also vectorized,
with the effect that each success count \code{y[i]} is drawn from a
binomial distribution with number of trials \code{n[i]} and chance of
success \code{theta[i]}.  In vectorized sampling statements, single
values may be repeated as many times as necessary.

The model block is executed after the transformed parameters block
every time the log probability function is evaluated. 

\subsubsection{Implicit uniform priors}\label{implicit-prior.section}

The default distribution for a variable is uniform over its
declared (constrained) support.  For instance, a variable declared with a lower
bound of 0 and an upper bound of 1 implicitly receives a $\mbox{\sf
  Uniform}(0,1)$ distribution.  These implicit uniform priors are
improper if the variable has unbounded support.  For instance, the
uniform distributions over real values with upper and lower bounds,
simplexes and correlation matrices is proper, but the uniform
distribution over unconstrained or one-side constrained reals, ordered
vectors or covariance matrices are not proper.

\proglang{Stan} does not require proper priors, but if the posterior
is improper, \proglang{Stan} will halt with an error message.%
%
\footnote{Improper posteriors are diagnosed automatically when
  parameters overflow to infinity during simulation.}
 

\subsection{Generated quantities block}

The (optional) generated quantities allows values that depend on
parameters and data, but do not affect estimation, to be defined
efficiently.  The generated quantities block is called only once per
sample, not once per log probability function evaluation.  It may be
used to calculate predictive inferences as well as to carry out
forward simulation for posterior predictive checks; see
\citep{GelmanEtAl:2013} for examples.

The \proglang{BUGS} surgical example explored the ranking of
institutions in terms of surgical mortality \citep[Examples, Volume
1]{LunnEtAl:2000}.  This is coded in the example in
Figure~\ref{hier-binom.fig} using the generated quantities block.  The
generated quantity variable \code{rnk[j]} will hold the rank of
institution \code{j} from 1 to \code{J} in terms of mortality rate
\code{theta[j]}.  The ranks are extracted using the \code{rank}
function. The posterior summary will print average rank and deviation.
\citep{LunnEtAl:2000} illustrated posterior inference by plotting
posterior rank histograms.

Posterior comparisons can be carried out directly or using rankings.
For instance, the model in Figure~\ref{hier-binom.fig} sets
\code{highest[j]} to 1 if hospital \code{j} has the highest estimated
mortality rate (for a discussion of multiple comparisions and
hierarchical models, see \citep{GelmanEtAl:2012, Efron:2010}).

As a second illustration, the generated quantities block n
Figure~\ref{hier-binom.fig} calculates the (posterior) probability
that a given institution is above-average in terms of mortality rate.
This is done for each institution \code{j} with the usual plug-in
estimate of \code{theta[j] > mean(theta)}, which returns a binary (0
or 1) value.  The posterior mean of \code{above\_avg[j]} calculates
the posterior probability $\mbox{Pr}[\theta_j > \bar{\theta}|y,n]$
according to the model.

\subsection{Initialization}

\proglang{Stan}'s samplers and optimizers all start from either random
or user-supplied values for each parameter.  User supplied initial
values are validated and transformed to the underlying unconstrained
space; if a parameter value does not satisfy its declared constraints,
the program exits and an informative error message is printed.  If
random initialization is specified, the built-in pseudorandom number
generator is called once per unconstrained variable dimension.  The
default initialization is to randomly generate values uniformly on
$[-2,2]$; another interval may be specified with \code{init=x} for
some non-negative floating-point value \code{x}.  This supplies fairly
diffuse starting points when transformed back to the constrained
scale, and thus help with convergence diagnostics as discussed in
\citep{GelmanEtAl:2013}.  Models with more data or more elaborate
structure require narrower intervals for initialization to ensure the
sampler is able to quickly converge to a stationary distribution in
the high mass region of the posterior.

Although \proglang{Stan} is quite effective at converging from diffuse
random initializations, the user may supply their own initial values
for sampling, optimization, or diagnosis.  The top-level command-line
option is \code{init=path}, where \code{path} is a path to a file
specifying values for all parameters in \proglang{R} dump format.




\section{Statements}\label{statements.section}

\subsection{Assignment and sampling}

\proglang{Stan} supports the same two basic statements as
\proglang{BUGS}, assignment and sampling, examples of which were
introduced earlier.  In \proglang{BUGS}, these two kinds of statment
define a directed acyclic graphical model; in \proglang{Stan}, they
define a log probability function. 

\subsubsection{Log probability accumulator}

There is an implicitly defined variable \code{lp\_\_} (available in 
the transformed parameters and model blocks) denoting the log
probability that will be returned by the log probability function.  

\subsubsection{Sampling statements}

A sampling statement is nothing more than shorthand for incrementing
the log probability accumulator \code{lp\_\_}.  For example, if
\code{beta} is a parameter of type \code{real}, the sampling statement
%
\begin{Code}
beta ~ normal(0,1);
\end{Code}
%
has the exact same effect (up to dropping constant terms) as the
special log probability increment statement
%
\begin{Code}
increment_log_prob(normal_log(beta,0,1));
\end{Code}


\subsubsection{Define variables before sampling statements}

The translation of sampling statements to log probability function
evaluations explains why variables must be defined {\it before} they
are used.  In particular, a sampling statement does \emph{not} sample
the left-hand side variable from the right-hand side distribution.

Parameters are all defined externally by the sampler; all other
variables must be explicitly defined with an assignment statement
before being used.


\subsubsection{Direct definition of probability functions}

Because computation is only up to a proportionality constant (an
additive constant on the log scale), this sampling statement in turn
has the same effect as the direct implementation in terms of basic
arithmetic,
%
\begin{Code}
increment_log_prob(-0.5 * beta * beta);
\end{Code}
%
If \code{beta} is of type \code{vector}, replace \code{beta * beta}
with \code{beta' * beta}.  Distributions whose probability functions
are not built directly into \proglang{Stan} can be implemented
directly in this fashion.



\subsection{Sequences of statements and execution order}

\proglang{Stan} allows sequences of statements wherever statements may
occur. Unlike \proglang{BUGS}, in which statements define a directed
acyclic graph, in \proglang{Stan}, statements are executed
imperatively in the order in which they occur in a program.

\subsubsection{Blocks and variable scope}

Sequences of statements surrounded by curly braces (\code{\{} and
\code{\}}) form blocks.  Blocks may start with local variable
declarations.  The scope of a local variable (i.e., where it is
available to be used) is that of the block in which it is declared.

Other variables, such as those declared as data or parameters, may
only be assigned to in the block in which they are declared.  They may
be used in the block in which they are declared and may also be used
in any block after the block in which they are declared.


\subsection{Whitespace, semicolons, and comments}

Following the convention of \proglang{C++}, statements are separated
with semicolons in \proglang{Stan} so that the content of whitespace
(outside of comments) is irrelevant.  This is in contrast to
\proglang{BUGS} and \proglang{R}, in which carriage returns are
special and may indicate the end of a statement.

\proglang{Stan} supports the line comment style of \proglang{C++},
using two forward slashes (\code{//}) to comment out the rest of a
line; this is the one location where the content of whitespace
matters.  \proglang{Stan} also supports the line comment style of \proglang{R} and
\proglang{BUGS}, treating a pound sign (\code{\#}) as commenting out
everything until the end of the line.  \proglang{Stan} also supports
\proglang{C++}-style block comments, with everything between the
start-comment (\code{/*}) and end-comment (\code{*/}) markers being
ignored. 

The preferred style follows that of \proglang{C++}, with line comment
used for everything but multiline comments.  

\proglang{Stan} follows the \proglang{C++} convention of separating words in
variable names using underbars (\code{\_}), rather than dots
(\code{.}), as used in \proglang{R} and \proglang{BUGS}, or camel case
as used in Java.

\subsection{Control structures}

\proglang{Stan} supports the same kind of explicitly bounded for loops
as found in \code{BUGS} and \code{R}.  Like \code{R}, but unlike
\proglang{BUGS}, \proglang{Stan} supports while loops and conditional
(if-then-else) statements.%
%
\footnote{\proglang{BUGS} omits these control structures because they
  would introduce ambiguities into the directed, acyclic graph defined
  by model.}
%
\proglang{Stan} provides the usual comparison operators and boolean
operators to help define conditionals and condition-controlled while
loops.  

\subsection{Print statements and debugging}

\proglang{Stan} provides print statements which take arbitrarily many
arguments consisting of expressions or string literals consisting of
sequences of characters surrounded by double quotes (\code{"}).
These statements may be used for debugging purposes to report on
intermediate states of variables or to indicate how far execution has
proceeded before an error.

As an example, suppose a user's program raises an error at run time
because a covariance matrix defined in the transformed parameters
block fails its symmetry constraint.  
%
\begin{Code}
transformed parameters {
  cov_matrix[K] Sigma;
  for (m in 1:M)
    for (n in m:M) 
      Sigma[m,n] <- Omega[m,n] * sigma[m] * sigma[n];

  print("Sigma=", Sigma);
}
\end{Code}
%
The print statement added at the last line will dump out the values in
the matrix. 


\section{Function and distribution library}\label{functions.section}

In order to support the algorithmic differentiation required to
calculate gradients, Hessians, and higher-order derivatives in
\proglang{Stan}, we require \proglang{C++} functions that are
templated separately on all of their arguments.  In order for these
functions to be efficient in computing both values and derivatives,
they need to operate directly on vectors of arguments so that shared
computations can be reused.  For example, if \code{y} is a vector and
\code{sigma} is a scalar, the logarithm of \code{sigma} need only be
evaluated once in order to compute the normal density for every member
of \code{y} in
%
\begin{Code}
y ~ normal(mu,sigma);
\end{Code}


\subsection{Basic operators}

\proglang{Stan} supports all of the basic \proglang{C++} arithmetic
operators, boolean operators, comparison operators  In addition, it
extends the arithmetic operators to matrices and includes pointwise
matrix operators.%
%
\footnote{This is in contrast to \proglang{R} and \proglang{BUGS}, who
  treat the basic multiplication and division operators pointwise and
  use special symbols for matrix operations.}
%
The full set of operators is listed in Figure~\ref{operators.fig}.

\subsection{Special functions}

\proglang{Stan} provides an especially rich set of special functions.
This includes all of the \proglang{C++} math library functions, as
well as numerous more specialized functions such as Bessel functions,
gamma and digamma functions, and generalized linear model link
functions and their inverses.  There are also many compound functions,
such as \code{log1m(x)}, which is more stable arithmetically for
values of x near 0 than \code{log(1 - x)}.  \proglang{Stan}'s special
functions are listed in Figure~\ref{special-functions.fig} and
Figure~\ref{special-functions-cont.fig}.  

In addition to special functions, \proglang{Stan} includes distributions with
alternative parameterizations, such as \code{bernoulli_logit}, which
takes a parameter on the log odds (i.e., logit) scale.  This allows a
more concise notation for generalized linear models as well as more
efficient and arithmetically stable execution.  


\subsection{Matrix and linear algebra functions}

Rows, columns and subblocks of matrices can be accessed using
\code{row}, \code{col}, and \code{block} functions.  Slices of arrays
can be accessed using the \code{head}, \code{tail}, and \code{segment}
functions.  There are also special functions for creating a diagonal
matrix from a vector and accessing the diagonal of a vector.

Various reductions are provided for arrays and matrices, such as sums,
means, standard deviations, and norms.  Replications are also available
to copy a value into every cell of a matrix.  Slices of matrices and
vectors may be accessed by row, column, or general sub-block
operations.

Matrix operators use the types of their operands to determine the type
of the result.  For instance, multiplying a vector by a (column) row
vector returns a matrix, whereas multiplying a row vector by a
(column) vector returns a real.  A postfix apostrophe (\code{'}) is
used for matrix and vector transposition.  For example, if \code{y}
and \code{mu} are vectors and \code{Sigma} is a square matrix, all of
the same dimensionality, then \code{y~-~mu} is a vector,
\code{(y~-~mu)'} is a row vector, \code{(y~-~mu)'~*~Sigma} is a row
vector, and \code{(y~-~mu)'~*~Sigma~*~(y~-~mu)} will be a real value.
Matrix division is provided, which is much more arithmetically stable
than inversion, e.g., \code{(y~-~mu)'~/~Sigma} computes the same
function as \code{(y~-~mu)'~*~inverse(Sigma)}.  \proglang{Stan} also
supports elementwise multiplication (\code{.*}) and division
(\code{./}).

Linear algebra functions are provided for trace, left and right
division, Cholesky factorization, determinants and log determinants,
inverses, eigenvalues and eigenvectors, and singular value
decomposition.  All of these operations may be applied to matrices of
parameters or constants.  Various functions are specialized for speed,
such as quadratic products, diagonal specializations, and multiply by
self transposed; e.g., the previous example
\code{(y~-~mu)'~*~Sigma~*~(y~-~mu)} could be coded as
as \code{quad\_form(Sigma,~y~-~mu)}.

The full set of matrix and linear-algebra functions is listed in
Figure~\ref{matrix-functions.fig}; operators, which also apply to
matrices and vectors, are listed in Figure~\ref{operators.fig}.

\subsection{Probability functions}

\proglang{Stan} supports a growing collection of built-in univariate and
multivariate probability density and mass functions.  These
probability functions share various features of their declarations and
behavior.

All probability functions are defined on the log scale to avoid
underflow.  They are all named with the suffix \code{\_log}, e.g.,
\code{normal\_log()}, is the log-scale normal distribution density
function.

All probability functions check that their arguments are within the
appropriate constrained support and are configured to throw exceptions
and print error messages for out-of-domain arguments (the behavior of
positive and negative infinity and not-a-number values are built into
floating-point arithmetic).  For example, \code{normal\_log(y,}
\code{mu,} \code{sigma)} requires the scale parameter \code{sigma} to
be non-negative.  Exceptions that are raised by functions will be
caught by the sampler, optimizer or diagnostic, and their warning
messages will be printed for the user.  Log density evaluations in
which exceptions are raised are treated as if they had evaluated to
negative infinity, and are thus rejected by the sampler or optimizer.

The list of probability functions is provided in
Figure~\ref{prob-functions.fig}, 
Figure~\ref{prob-functions-cont.fig}, and
Figure~\ref{prob-functions-cont-2.fig}. 

\subsubsection{Up to a proportion calculations}

All probability functions support calculating results up to a constant
proportion, which becomes an additive constant on the log scale.
Constancy here refers to being a numeric literal such as \code{1} or
\code{0.5}, a constant function such as \code{pi()}, data and
transformed data variables, or a function that only depends on
literals, constant functions or data variables.

Non-constants include parameters, transformed parameters, local
variables declared in the transformed parameters or model statements,
as well as any expression involving a non-constant.

Constant terms are dropped from probability function calculations at
the time the model is compiled, so there is no run-time overhead to
decide which expressions denote constants.%
%
\footnote{Both vector arguments and dropping constant terms are
  implemented in \proglang{C++} through template metaprograms that
  infer traits of template arguments to the probability functions.
  Whether to drop constants is configurable through a boolean template
  parameter on the log probability and derivative functions generated
  in \proglang{C++} for a model.}
%
For example, executing \code{y ~ normal(0,sigma)} only evaluates
\code{log(sigma)} if \code{sigma} is a parameter, transformed
parameter, or a local variable in the transformed parameters or model
block; that is, \code{log(sigma)} is not evaluated if \code{sigma} is
constant as defined above.  

Constant terms are {\it not} dropped in explicit function evaluations,
such as \code{normal\_log(y,0,sigma)}.

\subsubsection{Vector arguments and shared computations}

All of the univariate probability functions in \proglang{Stan} are
implemented so that they accept arrays or vectors of arguments.  For
example, although the basic signature of the probability function
\code{normal\_log(y,mu,sigma)} involves real \code{y}, \code{mu} and
\code{sigma}, it supports calls in which any any or all of \code{y},
\code{mu} and \code{sigma} contain more than one element. A typical
use case would be for linear regression, such as \code{y ~ normal(X *
  beta,sigma)}, where \code{y} is a vector of observed data, \code{X}
is a predictor matrix, \code{beta} is a coefficient vector, and
\code{sigma} is a real value for the noise scale.

The advantage of using vectors is twofold.  First, the models are more
concise and closer to mathematical notation.  Second, the vectorized
versions are much faster.  They reduce the number of times expensive
operations need to be evaluated and also reduce the number of virtual
function calls required in the compiled \proglang{C++} executable for
calculating gradients and higher-order derivatives.  For example if
\code{sigma} is a parameter, then in evaluating {y ~
  normal(X~*~beta,~sigma)}, the logarithm of \code{sigma} need only be
computed once; if either \code{y} or \code{beta} is an $N$-vector, it
also reduces the number of virtual function calls from $N$ to 1.

\section{Built-in inference engines}\label{inference-engines.section}

\proglang{Stan} includes several Markov chain Monte Carlo (MCMC)
samplers and several optimizers.  Others may be straightforwardly
implemented within \proglang{Stan}'s \proglang{C++} framework for
sampling and optimization using the log probability and derivative
information supplied by a model.

\subsection{Markov chain Monte Carlo samplers}

\subsubsection{Hamiltonian Monte Carlo}

The MCMC samplers provided include Euclidean manifold Hamiltonian
Monte Carlo (EHMC, or just HMC) \citep{DuaneEtAl:1987, Neal:1994,
  Neal:2011} and the no-U-turn sampler (NUTS)
\citep{HoffmanGelman:2011}.  Both the basic and NUTS versions of HMC
allow estimation or specification of unit, diagonal, or full mass
matrices.  NUTS, the default sampler for \proglang{Stan},
automatically adapts the number of leapfrog steps, eliminating the
need for user-specified tuning parameters.  Both algorithms take
advantage of gradient information in the log probability function to
generate coherent motion through the posterior that dramatically
reduces the autocorrelation of the resulting transitions.

\subsection{Optimizers}

In addition to performing full Bayesian inference via posterior
sampling, \proglang{Stan} also can perform optimization (i.e., computation of the
posterior mode).  We are currently working on implementing other
optimization-based inference approaches including variational Bayes,
expectation propagation, and and marginal inference using approximate
integration.  All these algorithms require optimization steps.

\subsubsection{BFGS}

The default optimizer in \proglang{Stan} is the
Broyden-Fletcher-Goldfarb-Shanno (BFGS) optimizer.  BFGS is a
quasi-Newton optimizer that evaluates gradients directly, then uses
the gradients to update an approximation to the Hessian.  Plans are in
the works to also include the more involved, but more scalable
limited-memory BFGS (L-BFGS) scheme.  \citet{NocedalWright:2006} cover
both BFGS and L-BFGS samplers.

\subsubsection{Conjugate gradient}

\proglang{Stan} provides a standard form of conjugate gradient
optimization; see \citep{NocedalWright:2006}.  As its name implies,
conjugate gradient optimization requires gradient evaluations.

\subsubsection{Accelerated gradient}

Additionally, \proglang{Stan} implements a crude version of Nesterov's
accelerated gradient optimizer \cite{Nesterov:1983}, which combines
gradient updates with a momentum-like update to hasten convergence.

% \section{Random number generation}\label{rng.section}

% Random number generation for \proglang{Stan} is done on a per-chain
% basis; ensemble samplers form a single joint chain over the ensemble.  By
% specifying the chain being used, the random number generator can be
% skipped ahead sufficiently to avoid replication of subsequences of
% random numbers across chains.

% The generated model code and underlying \proglang{C++} algorithms
% provide template parameters for a class implementing the \pkg{Boost}
% random number generator concept.  

% By default, \proglang{Stan} uses linear congruential generators
% \citep{LEcuyer:1988}.  This generator supports efficient skip-ahead.

% \subsection{Replicability}

% The \proglang{Stan} interfaces all allow random-number generator seeds
% to be specified explicitly.  Execution uses a single base random
% number generator instance.  Therefore, by specifying a seed,
% \proglang{Stan}'s behavior is deterministic.  This is very useful for
% debugging purposes.  Seeds can be generated randomly based on
% properties of the system time, but when they are, the seed used is
% printed as part of the output to allow it to be fully replicated.



\section*{Acknowledgments}

First and foremost, we would like to thank all of the users of
\proglang{Stan} for taking a chance on a new package and sticking with
it as we ironed out the details in the first release.  We'd like to
particularly single out the students in Andrew Gelman's Bayesian data
analysis courses at Columbia Univesity and Harvard University, who
served as trial subjects for both \proglang{Stan} and \citep{GelmanEtAl:2013}.

We'd like to thank Aki Vehtari for comments and corrections on a draft
of the paper.

\proglang{Stan} was and continues to be supported largely through grants from the
U.~S. government.  Grants which indirectly supported the initial
research and development included grants from the Department of Energy
(DE-SC0002099), the National Science Foundation (ATM-0934516), and the
Department of Education Institute of Education Sciences
(ED-GRANTS-032309-005 and R305D090006-09A).  The high-performance
computing facility on which we ran evaluations was made possible
through a grant from the National Institutes of Health
(1G20RR030893-01).  \proglang{Stan} is currently supported in part by a grant
from the National Science Foundation (CNS-1205516).

We would like to think those who have contributed code for new
features, Jeffrey Arnold, Yuanjun Gao, and Marco Inacio, as well as
those who contributed documentation corrections and code patches,
Jeffrey Arnold, David R. Blair, Eric C. Brown, Eric N. Brown, Devin
Caughey, Wayne Folta, Andrew Hunter, Marco Inacio, Louis Luangkesorn,
Jeffrey Oldham, Mike Ross, Terrance Savitsky, Yajuan Si, Dan Stowell,
Zhenming Su, and Dougal Sutherland.

Finally, we would like to thank two anonymous referees.

\begin{figure}
\begin{center}
\begin{tabular}{c|ccl|l}
{ Operation} & { Precedence} & { Associativity} & {\it
  Placement} & { Description}
\\ \hline \hline
\code{||} & 9 & left & binary infix & logical or
\\ \hline
\Verb|&&| & 8 & left & binary infix & logical and
\\ \hline
\Verb|==| & 7 & left & binary infix & equality
\\
\Verb|!=| & 7 & left & binary infix & inequality
\\ \hline
\Verb|<| & 6 & left & binary infix & less than
\\
\Verb|<=| & 6 & left & binary infix & less than or equal
\\
\Verb|>| & 6 & left & binary infix & greater than 
\\
\Verb|>=| & 6 & left & binary infix & greater than or equal
\\ \hline
\code{+} & 5 & left & binary infix & addition
\\
\code{-} & 5 & left & binary infix & subtraction
\\ \hline
\code{*} & 4 & left & binary infix & multiplication
\\
\code{/} & 4 & left & binary infix & (right) division
\\ \hline
\Verb|\| & 3 & left & binary infix & left division
\\ \hline
\code{.*} & 2 & left & binary infix & elementwise multiplication
\\
\code{./} & 2 & left & binary infix & elementwise division
\\ \hline
\code{!} & 1 & n/a & unary prefix & logical negation
\\
\code{-} & 1 & n/a & unary prefix & negation
\\ 
\code{+} & 1 & n/a & unary prefix & promotion (no-op in \proglang{Stan})
\\ \hline
\code{'} & 0 & n/a & unary postfix & transposition
\\ \hline \hline
\code{()} & 0 & n/a & prefix, wrap & function application
\\
\code{[]} & 0 & left & prefix, wrap & array, matrix indexing
\end{tabular}
\end{center}
\caption{\it Each of \proglang{Stan}'s unary and binary operators follow
  strict precedences, associativities, placement within an expression.  
  The operators are listed in order of precedence, from least tightly
  binded to most tightly binding.}\label{operators.fig}
\end{figure}
%


\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{ Function} & { Description} \\ \hline \hline
\code{e} &  base of natural logarithm \\ 
\code{epsilon} &  smallest positive number \\ 
\code{negative\_epsilon} &  largest negative value \\ 
\code{negative\_infinity} &  negative infinity \\ 
\code{not\_a\_number} &  not-a-number \\ 
\code{pi} &  $\pi$ \\
\code{positive\_infinity} &  positive infinity \\  
\code{sqrt2} &  square root of two \\ 
\end{tabular}
\end{center}
\caption{\it \proglang{Stan} implements a variety of useful constants.}\label{constants.fig}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{ Function} & { Description} \\ \hline \hline
\code{acos} &  arc cosine \\ 
\code{acosh} &  arc hyperbolic cosine \\ 
\code{asin} &  arc sine \\ 
\code{asinh} &  arc hyperbolic sine \\ 
\code{atan} &  arc tangent \\ 
\code{atan2} &  arc ratio tangent \\ 
\code{atanh} &  arc hyperbolic tangent \\ 
\code{cos} &  cosine \\ 
\code{cosh} &  hyperbolic cosine\\ 
\code{hypot} &  hypoteneuse \\ 
\code{sin} &  sine \\ 
\code{sinh} &  hyperbolic sine \\ 
\code{tan} &  tangent \\ 
\code{tanh} &  hyperbolic tangent \\ 
\end{tabular}
\end{center}
\caption{\it \proglang{Stan} implements both circular and 
  hyperbolic trigonometric functions, as well as their inverses.}
  \label{trig-functions-cont.fig}
\end{figure}


\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{ Function} & { Description}  \\ \hline \hline
\code{abs} &  double absolute value \\ 
\code{abs} &  integer absolute value \\ 
\code{binary\_log\_loss} &  log loss \\ 
\code{bessel\_first\_kind} & Bessel function of the first kind \\
\code{bessel\_second\_kind} & Bessel function of the second kind \\
\code{binomial\_coefficient\_log} & log binomial coefficient \\ 
\code{cbrt} &  cube root \\ 
\code{ceil} &  ceiling \\ 
\code{cumulative\_sum} &  cumulative sum \\ 
\code{erf} &  error function \\ 
\code{erfc} &  complementary error function \\ 
\code{exp} &  base-$e$ exponential \\ 
\code{exp2} &  base-2 exponential \\ 
\code{expm1} &  exponential of quantity minus one \\
\code{fabs} &  real absolute value \\ 
\code{fdim} &  positive difference \\ 
\code{floor} &  floor \\ 
\code{fma} &  fused multiply-add \\ 
\code{fmax} &  floating-point maximum \\ 
\code{fmin} &  floating-point minimum \\ 
\code{fmod} &  floating-point modulus \\ 
\code{if\_else} &  conditional \\ 
\code{int\_step} &  Heaviside step function \\ 
\code{inv} &  inverse (one over argument) \\ 
\code{inv\_cloglog} &  inverse of complementary log-log \\ 
\code{inv\_logit} &  logistic sigmoid \\ 
\code{inv\_sqrt} &  inverse square root \\ 
\code{inv\_square} &  inverse square \\ 
\code{lbeta} & log beta function \\ 
\code{lgamma} &  log $\Gamma$ function \\ 
\code{lmgamma} &  log multi-$\Gamma$ function \\ 
\code{log} &  natural (base-$e$) logarithm \\ 
\code{log10} &  base-10 logarithm \\ 
\code{log1m} &  natural logarithm of one minus argument \\ 
\code{log1m\_exp} &  natural logarithm of one minus natural exponential \\ 
\code{log1m\_inv\_logit} &  natural logarithm of logistic sigmoid \\ 
\code{log1p} &  natural logarithm of one plus argument \\ 
\code{log1p\_exp} &  natural logarithm of one plus natural exponential \\ 
\code{log2} &  base-2 logarithm \\ 
\end{tabular}
\end{center}
\caption{\it \proglang{Stan} implements many special and transcendental functions.}\label{special-functions.fig}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{ Function} & { Description} \\ \hline \hline
\code{log\_diff\_exp} & natural logarithm of difference of
exponentials \\ 
\code{log\_falling\_factorial} & falling factorial (Pochhammer) \\
\code{log\_inv\_logit} &  natural logarithm of the logistic sigmoid \\ 
\code{log\_rising\_factorial} & falling factorial (Pochhammer) \\
\code{log\_sum\_exp} &  logarithm of the sum of exponentials of arguments \\ 
\code{logit} &  log-odds \\ 
\code{max} &  integer maximum \\ 
\code{max} &  real maximum \\ 
\code{mean} &  sample average \\ 
\code{min} &  integer minimum \\ 
\code{min} &  real minimum \\ 
\code{modified\_bessel\_first\_kind} & modified Bessel function of the first kind \\
\code{modified\_bessel\_second\_kind} & modified Bessel function of the second kind \\
\code{multiply\_log} &  multiply linear by log \\ 
\code{owens\_t} &  Owens-t \\ 
\code{phi} &  $\Phi$ function (cumulative unit normal) \\ 
\code{phi\_approx} &  efficient, approximate $\Phi$ \\ 
\code{pow} &  power (i.e., exponentiatiation) \\
\code{prod} &  product of sequence \\
\code{rank} &  rank of element in array or vector \\ 
\code{rep\_array} &  fill array with value \\
\code{round} &  round to nearest integer \\ 
\code{sd} &  sample standard deviation \\ 
\code{softmax} &  softmax (multi-logit link) \\ 
\code{sort\_asc} &  sort in ascending order \\ 
\code{sort\_desc} &  sort in descending order \\ 
\code{sqrt} &  square root \\ 
\code{square} &  square \\ 
\code{step} &  Heaviside step function \\ 
\code{sum} &  sum of sequence \\ 
\code{tgamma} &  $\Gamma$ function \\ 
\code{trunc} & truncate real to integer \\ 
\code{variance} &  sample variance \\ 
\end{tabular}
\end{center}
\caption{\it Special functions (continued).}\label{special-functions-cont.fig}
\end{figure}



\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{ Function} & { Description} \\ \hline \hline
\code{block} &  sub-block of matrix \\
\code{cholesky\_decompose} &  Cholesky decomposition \\ 
\code{col} &  column of matrix \\ 
\code{cols} &  number of columns in matrix \\ 
\code{columns\_dot\_product} &  dot product of matrix columns \\ 
\code{columns\_dot\_self} &  dot product of matrix columns with self \\ 
\code{crossprod} &  cross-product \\ 
\code{determinant} &  matrix determinant \\ 
\code{diag\_matrix} &  vector to diagonal matrix \\ 
\code{diag\_post\_multiply} &  post-multiply matrix by diagonal matrix \\ 
\code{diag\_post\_multiply} &  pre-multiply matrix by diagonal matrix \\ 
\code{diagonal} &  diagonal of matrix as vector \\ 
\code{dims} &  dimensions of matrix \\ 
\code{dot\_product} &  dot product \\ 
\code{dot\_self} &  dot product with self \\ 
\code{eigenvalues\_sym} &  eigenvalues of symmetric matrix \\ 
\code{eigenvectors\_sym} &  eigenvectors of symmetric matrix \\ 
\code{head} &  head of vector \\
\code{inverse} &  matrix inverse \\ 
\code{inverse\_spd} & symmetric, positive-definite matrix inverse \\ 
\code{log\_determinant} &  natural logarithm of determinant \\ 
\code{mdivide\_left\_tri\_low} &  lower-triangular matrix left division \\ 
\code{mdivide\_right\_tri\_low} &  lower-triangular matrix right division \\
\code{multiply\_lower\_tri\_self\_transpose} &  multiply
lower-triangular by transpose \\ 
\code{quad\_form} &  quadratic form vector-matrix multiplication \\ 
\code{rep\_matrix} &  replicate scalar, row vector or vector to matrix \\ 
\code{rep\_row\_vector} &  replicate scalar to row vector \\ 
\code{rep\_vector} &  replicate scalar to vector \\ 
\code{row} &  row of matrix \\ 
\code{rows} &  number of rows in matrix \\ 
\code{rows\_dot\_product} &  dot-product of rows of matrices \\ 
\code{rows\_dot\_self} &  dot-product of matrix with itself \\ 
\code{segment} &  sub-vector \\ 
\code{singular\_values} &  singular values of matrix \\ 
\code{size} &  number of entries in array or vector \\ 
\code{sub\_col} &  sub-column of matrix \\ 
\code{sub\_row} &  sub-row of matrix \\ 
\code{tail} &  tail of vector \\ 
\code{tcrossprod} &  matrix post-multiply by own transpose \\ 
\code{trace} &  trace of matrix \\ 
\code{trace\_gen\_quad\_form} & trace of generalized quadratic form \\ 
\code{trace\_quad\_form} &  trace of quadratic form
\end{tabular}
\end{center}
\caption{\it A large suite of matrix functions admits efficient 
  multivariate model implementation in \proglang{Stan}.}
  \label{matrix-functions.fig}
\end{figure}


\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{ Function} & { Description} \\ \hline \hline
\code{bernoulli\_cdf} &  Bernoulli cdf \\ 
\code{bernoulli\_log} &  log Bernoulli pmf \\ 
\code{bernoulli\_logit\_log} &  logit-scale log Bernoulli pmf \\ 
\code{bernoulli\_rng} &  Bernoulli RNG \\ 
\code{beta\_binomial\_cdf} &  beta-binomial cdf \\ 
\code{beta\_binomial\_log} &  log beta-binomial pmf \\ 
\code{beta\_binomial\_rng} &  beta-binomial rng \\ 
\code{beta\_cdf} &  beta cdf \\ 
\code{beta\_log} &  log beta pdf \\ 
\code{beta\_rng} &  beta RNG \\ 
\code{binomial\_cdf} &  binomial cdf n \\ 
\code{binomial\_log} &  log binomial pmf \\ 
\code{binomial\_logit\_log} &  log logit-scaled binomial pmf \\ 
\code{binomial\_rng} &  binomail RNG \\ 
\code{categorical\_log} &  log categorical pmf \\ 
\code{categorical\_rng} &  categorical RNG \\ 
\code{cauchy\_cdf} &  Cauchy cdf \\ 
\code{cauchy\_log} &  log Cauchy pdf \\ 
\code{cauchy\_rng} &  Cauchy RNG \\ 
\code{chi\_square\_log} &  log chi-square pdf \\ 
\code{chi\_square\_rng} &  chi-square RNG \\ 
\code{dirichlet\_log} &  log Dirichlet pdf \\ 
\code{dirichlet\_rng} &  Dirichlet RNG \\ 
\code{double\_exponential\_log} &  log double-exponential (Laplace) pdf \\ 
\code{double\_exponential\_rng} &  double-exponential (Laplace) RNG \\ 
\code{exp\_mod\_normal\_cdf} &  exponentially modified normal cdf \\
\code{exp\_mod\_normal\_log} &  log exponentially modified normal pdf \\ 
\code{exp\_mod\_normal\_rng} &  exponentially modified normal RNG \\ 
\code{exponential\_cdf} &  exponentia cdf \\ 
\code{exponential\_log} &  log of exponential pdf \\ 
\code{exponential\_rng} &  exponential RNG \\ 
\code{gamma\_log} &  log gamma pdf \\ 
\code{gamma\_rng} &  gamma RNG \\ 
\code{gumbel\_cdf} &  Gumbel cdf \\ 
\code{gumbel\_log} &  log Gumbel pdf \\ 
\code{gumbel\_rng} &  Gumbel RNG\\ 
\code{hypergeometric\_log} &  log hypergeometric pmf \\ 
\code{hypergeometric\_rng} &  hypergeometric RNG \\ 
\code{inv\_chi\_square\_cdf} &  inverse chi-square cdf \\ 
\code{inv\_chi\_square\_log} &  log inverse chi-square pdf \\ 
\code{inv\_chi\_square\_rng} &  inverse chi-square RNG \\ 
\end{tabular}
\end{center}
\caption{\it Most common probability distributions have
  explicitly implemented in \proglang{Stan}.}\label{prob-functions.fig}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{ Function} & { Description} \\ \hline \hline
\code{inv\_gamma\_cdf} & inverse gamma cdf \\ 
\code{inv\_gamma\_log} &  log inverse gamma pdf \\ 
\code{inv\_gamma\_rng} &  inverse gamma RNG \\ 
\code{inv\_wishart\_log} &  log inverse Wishart pdf \\ 
\code{inv\_wishart\_rng} &  inverse Wishart RNG \\ 
\code{lkj\_corr\_cholesky\_log} &  log LKJ correlation, Cholesky-variate pdf \\ 
\code{lkj\_corr\_cholesky\_rng} &  LKJ correlation, Cholesky-variate RNG \\
\code{lkj\_corr\_log} &  log of LKJ correlation pdf \\ 
\code{lkj\_corr\_rng} &  LKJ correlation RNG \\ 
% \code{lkj\_cov\_log} &  {\it deprecated} \\ 
\code{logistic\_cdf} &  logistic cdf \\ 
\code{logistic\_log} &  log logistic pdf \\ 
\code{logistic\_rng} &  logistic RNG \\ 
\code{lognormal\_cdf} &  lognormal cdf \\ 
\code{lognormal\_log} &  log of lognormal pdf \\ 
\code{lognormal\_rng} &  lognormal RNG \\ 
\code{multi\_normal\_cholesky\_log} & log multi-normal Cholesky-parameterized pdf  \\ 
\code{multi\_normal\_log} &  log multi-normal pdf \\ 
\code{multi\_normal\_prec\_log} & log multi-normal precision-parameterized pdf \\ 
\code{multi\_normal\_rng} &  multi-normal RNG \\ 
\code{multi\_student\_t\_log} & log multi student-t pdf \\
\code{multi\_student\_t\_rng} &  multi student-t RNG \\ 
\code{multinomial\_log} &  log multinomial pmf \\ 
\code{multinomial\_rng} &  multinomial RNG \\ 
\code{neg\_binomial\_cdf} &  negative binomial cdf \\ 
\code{neg\_binomial\_log} &  log negative binomial pmf \\ 
\code{neg\_binomial\_rng} &  negative biomial RNG \\ 
\code{normal\_cdf} &  normal cdf \\ 
\code{normal\_log} &  log normal pdf (c.f. log lognormal pdf) \\ 
\code{normal\_rng} &  normal RNG \\ 
\code{ordered\_logistic\_log} &  log ordinal logistic pmf \\ 
\code{ordered\_logistic\_rng} &  ordinal logistic RNG \\ 
\code{pareto\_cdf} &  Pareto cdf \\ 
\code{pareto\_log} &  log Pareto pdf \\ 
\code{pareto\_rng} &  Pareto RNG \\ 
\code{poisson\_cdf} &  Poisson cdf \\ 
\code{poisson\_log} &  log Poisson pmf \\ 
\code{poisson\_log\_log} &  log Poisson log-parameter pdf \\ 
\code{poisson\_rng} &  Poisson RNG \\ 
\end{tabular}
\end{center}
\caption{\it Probability functions (continued)}\label{prob-functions-cont.fig}
\end{figure}


\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{ Function} & { Description} \\ \hline \hline
\code{rayleigh\_log} & log Rayleigh pdf \\
\code{rayleigh\_rng} & Rayleigh RNG \\
\code{rayleigh\_cdf} & Rayleigh cdf \\
\code{scaled\_inv\_chi\_square\_cdf} &  scaled inverse-chi-square cdf \\ 
\code{scaled\_inv\_chi\_square\_log} &  log scaled inverse-chi-square pdf \\ 
\code{scaled\_inv\_chi\_square\_rng} &  scaled inverse-chi-square RNG \\ 
\code{skew\_normal\_cdf} &  skew-normal cdf \\
\code{skew\_normal\_log} &  log of skew-normal pdf \\
\code{skew\_normal\_rng} &  skew-normal RNG \\ 
\code{student\_t\_cdf} &  Student-t cdf \\
\code{student\_t\_log} &  log of Student-t pdf \\
\code{student\_t\_rng} &  Student-t RNG \\ 
\code{uniform\_log} &  log of uniform pdf \\ 
\code{uniform\_rng} &  uniform RNG \\ 
\code{weibull\_cdf} &  Weibull cdf \\ 
\code{weibull\_log} &  log of Weibull pdf \\ 
\code{weibull\_rng} &  Weibull RNG \\ 
\code{wishart\_log} &  log of Wishart pdf \\ 
\code{wishart\_rng} &  Wishart RNG \\ 
\end{tabular}
\end{center}
\caption{\it Probability functions (continued 2)}\label{prob-functions-cont-2.fig}
\end{figure}


\appendix

\section{Developer process}\label{process.section}

\subsection{Version control and source repository}

\proglang{Stan}'s source code is hosted on GitHub and managed using
the \pkg{Git} version control system \citep{Chacon:2009}.  To manage
the workflow with so many developers working at any given time, the
project follows the GitFlow process \citep{Driessen:2010}.  All
developer submissions are managed through pull requests and we have
gratefully received patches from numerous sources outside the core
development team.


\subsection{Continuous integration}

\proglang{Stan} uses continuous integration, meaning that the entire
program and set of tests are run automatically as code is pushed to
the \pkg{Git} repository.  Each pull request is tested for
compatibility with the development branch, and the development branch
itself is tested for stability.  \proglang{Stan} uses Jenkins
\citep{Smart:2011}, an open-source continuous integration server.

\subsection{Testing framework}

\proglang{Stan} includes extensive unit tests for low-level \proglang{C++} code.
Unit tests are implemented using the \pkg{googletest} framework
\citep{GoogleTest:2011}.  The probability functions and command-line
invocations are complex enough that programs are used to automatically
generate test code for \pkg{googletest}.

These unit tests evaluate every function for both appropriate values
and appropriate derivatives.  This requires an extensive meta-testing
framework for the probability distributions due to their high degree
of configurability as to argument types.  The testing portion of the
makefile also runs tests of all of the built-in models, including
almost all of the \proglang{BUGS} sample models.  Models are tested
for both convergence and posterior mean estimation to within MCMC
standard error.


\subsection{Builds}

The build process for \proglang{Stan} is highly automated through a
cross-platform series of \code{make} files.  The top-level makefile
builds the \proglang{Stan}-to-\proglang{C++} translator command
\code{bin/stanc} and posterior analysis command \code{bin/print}.  It
also builds the library archive \code{bin/libstan.a}.  Great care was
taken to avoid complicated platform-dependent configuration
requirements that place a high burden on user system knowledge for
installation.  All that is needed is a relatively recent
\proglang{C++} compiler and version of \code{make}.

As exemplified in the introduction, the makefile is automated enough
to build an executable form of a \proglang{Stan} model in a single
command.  All libraries and other executables will be built as a side
effect.  

The top-level makefile also supplies targets to build all of the
documentation \proglang{C++} API documentation is generated using the
\pkg{doxygen} package \citep{Doxygen:2011}.  The \proglang{Stan}
manual \citep{Stan:2013} is typeset using the \LaTeX\ package
\citep{MittelbachEtAl:2004}.

The makefile also has targets for all of the unit and functional
testing, for building the source-level distribution, and for cleaning
any temporary files that it creates.

\section{Library dependencies}\label{libraries.section}

\proglang{Stan}'s modeling language is only dependent on two external
libraries.  

\subsection{Boost} 

\proglang{Stan} depends on several of the \pkg{Boost} \proglang{C++}
libraries \citep{Boost:2011}.  \proglang{Stan} makes extensive use of
\pkg{Boost}'s template metaprogramming facilities including the
\pkg{Enable if} package, the \pkg{Type Traits} library, and the
\pkg{Lexical Cast} library.  The \proglang{Stan} language is parsed
using \pkg{Boost}'s \pkg{Spirit} parser, which itself depends on
the binding libraries \pkg{Phoenix}, \pkg{Bind}, and \pkg{Lambda},
the variant type library \pkg{Variant}, and the container library
\pkg{Fusion}.  Exceptions are largely handled and configured through
the error checking facilities in the \pkg{Math} and
\pkg{Exception} packages.  Output formatting and ad-hoc input parsing
for various formats is facilitated with the \pkg{Format} library.
\proglang{Stan} relies heavily on the special functions defined in the
\pkg{Math} subpackages \pkg{Special Functions} and \pkg{Statistical
  Distributions}.  Random number generation is carried out using the
\pkg{Random} package.  The posterior analysis framework and some
built-in functions depend on the \pkg{Accumulators} package.

\subsection{Eigen}

\proglang{Stan}'s handling of matrices and linear algebra is
implemented through the \pkg{Eigen} \proglang{C++} template library
\citep{Eigen:2012}.  Eigen uses template metaprogramming to achieve
state-of-the-art performance for matrix and linear algebra operations
with a great deal of flexiblity with respect to input types.
Unfortunately, many of the expression templates that Eigen uses for
efficient static anaysis and lazy evaluation are short-circuited
because of \proglang{Stan}'s need to have mixed type operations (i.e.,
multiplying a constant predictor matrix of double values by a
parameter vector of algorithmic differentiation values).  To make up for
this in some important cases, \proglang{Stan} has provided compound
functions such as the quadratic form, which allow speedups of both the
matrix operations and their derivatives compared to a direct
implementation using \proglang{Stan}'s built-in operators.









% \nocite{WainwrightJordan:2008}
\nocite{R:2013}
\clearpage
% implicit: \bibliographystyle{jss}
\bibliography{stan-paper}



















 




\end{document}


