\documentclass[10pt]{article}
\usepackage{stan-papers}

\title{\Large\bfseries The \code{\bfseries stan::agrad} C++ Automatic
  Differentiation Library}

\author{\large Bob Carpenter \\ {\small Columbia University}
        \\[8pt]
        \large Marcus Brubaker \\ {\small Toyota Technical Institute}
   \and \large Matt Hoffman \\ {\small Adobe Research Labs}
        \\[8pt]
        \large Peter Li \\ {\small Columbia University}
   \and \large Daniel Lee \\ {\small Columbia University}
        \\[8pt]
        \large Michael Betancourt \\ {\small University of Warwick}
}
\date{\vspace*{8pt}\normalsize \today}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract} 
  \noindent
  The \code{stan::agrad} C++, reverse-mode automatic differentiation
  library was designed to be usable, extensive and extensibile,
  efficient, scalable, stable, portable, and redistributable.

  Usability is achieved through a simple direct interface and a
  cleanly abstracted functional interface.  The extensive built-in
  library includes functions for matrix operations, linear algebra,
  differential equation solving, and most common probability
  functions.  Extensibility derives from a straightforward
  object-oriented framework for expressions, allowing developers and
  users to easily create custom functions. Efficiency is achieved
  through a combination of custom memory management, subexpression
  caching, traits-based metaprogramming, and expression templates.
  Partial derivatives for compound functions are evaluated lazily for
  improved scalability.  Stability is achieved by taking care with
  arithmetic precision in algebraic expressions and providing stable,
  compound functions where possible. For portability, the library is
  standards-compliant C++ (03) and has been tested for all major
  compilers for Windows, Mac OS X, and Linux.  It is distributed under
  the new BSD license.

  This paper provides an overview of \code{agrad}'s application
  programmer interface (API), examples of its use, and a thorough
  explanation of how it is implemented.  It also demonstrates the
  efficiency and scalability of \code{agrad} by comparing its speed
  and memory usage of gradient calculations to that of several popular
  open-source C++ automatic differentiation systems (Adept, CppAD, and
  Sacado), with results varying dramatically according to the type of
  expression being differentiated.
\end{abstract}

\clearpage

\section{Reverse-Mode Automatic Differentiation}

Reverse-mode automatic differentiation operates on an arbitrary
differentiable mathematical function expressed as a computer program
and a specified set of inputs to calculate the derivatives of the
function output with respect to the inputs.  The inputs are
independent variables, the output a dependent variable, and the vector
of derivatives is the gradient evaluated at the independent variables.

For example, automatic differentiation will take a simple C++
expression expression such as \code{x~*~y~/~2} and inputs such as
$\code{x}=6$ and $\code{y}=4$ and produce the output value 12 and pair
of derivatives $(2,3)$ of the result with respect to \code{x} and
\code{y}.  

Reverse-mode automatic differentiation constructs an expression graph
and propagating the chain rule.  In somewhat more detail, it works by
%
\begin{enumerate}
\item converting the formula to a rooted, directed, acyclic expression graph,
  where parent nodes represent function values and child nodes
  represent operands to the function,
\item labeling each node with its value and each edge with 
  the partial derivative of the parent node with respect to the child
  node, 
\item setting the root node's adjoint  to 1 and all other nodes' adjoints
  to 0, 
\item traversing the expression graph top down, visiting each node
  before any of its child nodes, and for each node and each of its
  child nodes, incrementing the child node's adjoint by the product of
  the parent node's adjoint and the partial derivative of the parent
  with respect to the child,
\item noticing that the adjoints hold the derivative of the root node
  with respect to each node, and
\item reading the gradient from the input nodes' adjoints.
\end{enumerate}
%

\subsection{Mechanics of Reverse Mode Automatic Differentiation}

As an example, consider the normal log probability density function
for a variable $y$ with a normal distribution with mean $\mu$ and
standard deviation $\sigma$,
%
\renewcommand{\theequation}{\arabic{equation}}
\begin{equation}\label{normal-log-density.equation}
\log \left( \distro{Normal}(y|\mu,\sigma) \right)
= -\frac{1}{2} \left( \frac{y - \mu}{\sigma} \right)^2
- \log \sigma
- \frac{1}{2} \log (2 \pi)
\end{equation}
%
and its gradient evaluated a point
%
\begin{equation}\label{gradient-normal-log-density.equation}
  \nabla_{\! [y \, \mu \, \sigma]} \log \distro{Normal}(y|\mu,\sigma) 
  = 
  \left[
    -(y - \mu) \sigma^{-2} \ \ \ \
    (y - \mu) \sigma^{-2} \ \ \ \ 
    (y - \mu)^2 \sigma^{-3} - \sigma^{-1}
  \right]
\end{equation}


\newcommand{\gmnode}[3]{\put(#1,#2){\circle{20}}\put(#1,#2){\makebox(0,0){$#3$}}}
\newcommand{\gmnodeobs}[3]{\put(#1,#2){\color{yellow}\circle*{20}}\put(#1,#2){\color{black}\circle{20}}\put(#1,#2){\makebox(0,0){$#3$}}}
\newcommand{\gmnoderoot}[3]{\put(#1,#2){\color{red}\circle*{20}}\put(#1,#2){\color{black}\circle{20}}\put(#1,#2){\makebox(0,0){$#3$}}}
\newcommand{\gmdata}[3]{\put(#1,#2){\makebox(16,16){\footnotesize $#3$}}}
\begin{figure}
\begin{center}
\begin{picture}(200,200)
\gmnoderoot{130}{200}{-}
\put(142,200){\mbox{\color{blue}{\footnotesize $v_{10}$}}}
\gmnode{100}{170}{-}
\put(112,170){\mbox{\color{blue}{\footnotesize $v_{9}$}}}
\gmdata{150}{160}{\mbox{\color{gray}{$0.5\log 2 \pi$}}}
\gmnode{70}{140}{*}
\put(82,140){\mbox{\color{blue}{\footnotesize $v_{7}$}}}
\gmdata{30}{100}{\mbox{\color{gray}{$-.5$}}}
\gmnode{100}{110}{\mbox{\footnotesize pow}}
\put(112,110){\mbox{\color{blue}{\footnotesize $v_{6}$}}}
\gmdata{120}{70}{\mbox{\color{gray}{$2$}}}
\gmnode{190}{80}{\mbox{\footnotesize $\log$}}
\put(202,80){\mbox{\color{blue}{\footnotesize $v_{8}$}}}
\gmnode{70}{80}{/}
\put(82,80){\mbox{\color{blue}{\footnotesize $v_{5}$}}}
\gmnode{40}{50}{-}
\put(52,50){\mbox{\color{blue}{\footnotesize $v_{4}$}}}
\gmnodeobs{10}{20}{\mbox{\footnotesize $y$}}
\put(22,20){\mbox{\color{blue}{\footnotesize $v_{1}$}}}
\gmnodeobs{70}{20}{\mbox{\footnotesize $\mu$}}
\put(82,20){\mbox{\color{blue}{\footnotesize $v_{2}$}}}
\gmnodeobs{130}{20}{\mbox{\footnotesize $\sigma$}}
\put(142,20){\mbox{\color{blue}{\footnotesize $v_{3}$}}}
%
\put(123,193){\vector(-1,-1){16}}
\put(137,193){\color{gray}{\vector(1,-1){16}}}
\put(93,163){\vector(-1,-1){16}}
\put(107,163){\vector(1,-1){76}}
\put(63,133){\color{gray}{\vector(-1,-1){16}}}
\put(77,133){\vector(1,-1){16}}
\put(93,103){{\vector(-1,-1){16}}}
\put(107,103){\color{gray}{\vector(1,-1){16}}}
\put(63,73){\vector(-1,-1){16}}
\put(77,73){\vector(1,-1){46}}
\put(183,73){\vector(-1,-1){46}}
\put(33,43){\vector(-1,-1){16}}
\put(47,43){\vector(1,-1){16}}
\end{picture}
\end{center}
\vspace*{-12pt}
\caption{\small\it Expression graph for the normal log density
  function given in \refeq{normal-log-density}.  Each circle
  corresponds to an automatic differentiation variable, with the
  variable name given to the right in blue.  The independent variables
  are highlighted in yellow on the bottom row, with the final node
  highlighted in red on the top of the graph.  The function producing
  each node is displayed inside the circle, with operands denoted by
  arrows.  Constants are shown in gray with gray arrows leading to
  them because derivatives need not be propagated to constant
  operands.}\label{expression-graph.figure}
\end{figure}
%
This mathematical formula corresponds to the expression graph in
\reffigure{expression-graph}.  Each subexpression corresponds to a
node in the graph, and each edge connects the node representing a
function evaluation to its operands.  
%
\begin{figure}
\begin{center}
\begin{tabular}{c||c|cc}
{\it var} & {\it value} & \multicolumn{2}{|c}{\it partials}
\\ \hline \hline
$v_1$ & $y$ 
\\[2pt]
$v_2$ & $\mu$
\\[2pt]
$v_3$ & $\sigma$
\\[2pt]
$v_4$ & $v_1 - v_2$ & $\partial v_4 \partial v_1 = 1$ 
                   & $\partial v_4 / \partial v_2 = -1$
\\[4pt]
$v_5$ & $v_4 / v_3$ & $\partial v_5 / \partial v_4 = 1/v_3$
                    & $\partial v_5 / \partial v_3 = -v_4 v_3^{-2}$
\\[4pt]
$v_6$ & $\left(v_5\right)^2$
      & \multicolumn{2}{c}{$\partial v_6 / \partial v_5 = 2 v_5$}
\\[4pt]
$v_7$ & $(-0.5) v_6$ & \multicolumn{2}{c}{$\partial v_7 / \partial v_6
                                          = -0.5$}
\\[4pt]
$v_8$ & $\log v_3$ & \multicolumn{2}{c}{$\partial v_8 / \partial v_3 = 1/v_3$}
\\[4pt]
$v_9$ & $v_7 - v_8$ & $\partial v_9 / \partial v_7 = 1$
                    & $\partial v_9 / \partial v_8 = -1$
\\[4pt]
$v_{10}$ & $v_9 - (0.5 \log 2\pi)$ 
         & \multicolumn{2}{c}{$\partial v_{10} / \partial v_9 = 1$}
\end{tabular}
\end{center}
\caption{\small\it Example of gradient calculation for the the log
  density function of a normally distributed variable, as defined in
  \refeq{normal-log-density}.  In the forward pass, as the stack is
  constructed from the first input $v_1$ up to the final output
  $v_{10}$, the values of the variables and partials are computed
  numerically according to the formulas given in the value and
  partials columns of the table.}\label{forward-pass.figure}
\end{figure}

\reffigure{forward-pass} illustrates the forward pass used by
reverse-mode automatic differentiation to construct the expression
graph for a program.  The expression graph is constructed in the
ordinary evaluation order, with each subexpression being numbered and
placed on a stack.  The stack is initialized here with the dependent
variables, but this is not required.  Each operand to an expression is
evaluated before the expression node is created and placed on the
stack.  As a result, the stack provides a topological sort of the
nodes in the graph (i.e., a sort in which each node representing an
expression occurs above its subexpression nodes in the stack---see
\citep[Section~2.2.3]{knuth:97}).  \reffigure{forward-pass} lists in
the right column for each node, the partial derivative of the function
represented by the node with respect to its operands.  In
\code{stan::agrad}, most of these partials are evaluated lazily during
the reverse pass based on function's value and its operands' values.


\begin{figure}
\[
\begin{array}{rcl|l}
{\it var} & {\it operation} & {\it value} & {\it result}
\\ \hline \hline
a_{1:9} & = & 0 & a_{1:9} = 0
\\
a_{10} & = & 1 & a_{10} = 1
\\ \hline
a_{9} & {+}{=} & a_{10} \times (1) & a_9 = 1
\\
a_{7} & {+}{=} & a_9 \times (1) & a_7 = 1
\\
a_{8} & {+}{=} & a_9 \times (-1) & a_8 = -1
\\
a_{3} & {+}{=} & a_8 \times (1 / v_3) & a_3 = -1 / v_3
\\
a_{6} & {+}{=} & a_7 \times (-0.5) & a_6 = -0.5
\\
a_{5} & {+}{=} & a_6 \times (2 v_5) & a_5 = -v_5
\\
a_{4} & {+}{=} & a_5 \times (1 / v_3) & a_4 = -v_5 / v_3
\\
a_{3} & {+}{=} & a_5 \times (-v_4 v_3^{-2}) & a_3 = -1 / v_3 + v_5 v_4 v_3^{-2}
\\
a_{1} & {+}{=} & a_4 \times (1) & a_1 = -v_5 / v_3
\\
a_{2} & {+}{=} & a_4 \times (-1) & a_2 = v_5 / v_3
\end{array}
\]
\vspace*{-12pt}
\caption{\small\it In the reverse pass, the stack is
  traversed from the final output down to the inputs, and as each
  variable is visited, each of its operands is updated with the
  variable's adjoint times the partial with respect to the operand.
  After the reverse pass finishes, $(a_1,a_2,a_3)$ is the gradient of
  the density function evaluated at $(y,\mu,\sigma)$, which matches
  the correct result given in \refeq{gradient-normal-log-density}
  after substitution for $v_4$ and $v_5$.}
  \label{autodiff-stack.figure}
\end{figure}
%
\reffigure{autodiff-stack} shows the processing for reverse mode,
which involves an adjoint value for each node.  The adjoints for all
nodes other than the root are initialized to 0; the root's adjoint is
initialized to 1, becuase $\partial x / \partial x = 1$.  The backward
sweep simply walks down the expression stack, and for each node,
propagates derivatives from it down to its operands using the chain
rule.  Because the nodes are in topological order, by the time a node
is visted, its adjoint will represent the partial derivative of the
root of the overall expression with respect to the expression
represented by the node.  Each node then propagates its derivatives to
its operands by incrementing its operands' adjoints by the product of
the expression node's adjoint times the partial with respect to the
operand.  Thus when the reverse pass is completed, the adjoints of the
independent variables hold the gradient of the dependent variable
(function value) with respect to the independent variables (inputs).

\subsection{Comparison to Alternative Methods of Computing Gradients}

\subsubsection{Finite Differencing}

Finite differences is a method to approximate derivatives numerically
rather than symbolically.  Given a positive difference $\epsilon > 0$,
an approximate derivative can be calculated as
\[
\frac{\partial}{\partial x_n} f(x) 
\approx 
\frac{f(x_1,\ldots,x_n + \epsilon, \ldots, x_N) - f(x_1, \ldots, x_N)}
     {\epsilon}
\]
or a bit more accurately with a centered interval as
\[
\frac{\partial}{\partial x_n} f(x) 
\approx 
\frac{f(x_1,\ldots,x_n + \epsilon/2, \ldots, x_N) 
      - f(x_1,\ldots,x_n - \epsilon/2, \ldots, x_N)}
     {\epsilon}.
\]
%
Although straightforward and general, calculating gradients using
finite differences is slow and imprecise.  Finite differencing is slow
because it requires $N + 1$ function evaluations ($2N + 1$ in the
centered case) to compute the gradient of an $N$-ary function.  The
numerical issues with finite differencing arise because a small
$\epsilon$ is required for a good approximation of the derivative, but
a large $\epsilon$ is required for floating-point precision.  Small
$\epsilon$ values are problematic for accuracy because because
subtraction of differently scaled numbers loses a degree of precision
equal to their difference in scales; for example, subtracting a number
on the scale of $10^{-6}$ from a number on the scale of 1 loses 6
orders of precision in the $10^{-6}$-scaled term \citep{higham:2002}.
Thus in practice, the arithmetic precision of finite differencing is
usually at best $10^{-7}$ rather than the maximum of roughly
$10^{-14}$ possible with double-precision floating point calculations.

\subsubsection{Symbolic Differentiation}

Unlike symbolic computer algebra systems, such as Mathematica
\citep{mathematica:2014} or SymPy \citep{sympy:2014}, automatic
differentiation does not explicitly generate a mathematical expression
corresponding to the derivative calculation.  Rather, it calculates
the gradient analytically by applying the chain rule directly to the
original function's subexpressions.





\section{Calculating Gradients and Jacobians with \code{\bfseries
    stan::agrad}}

Reverse-mode automatic differentiation in \code{stan::agrad} can be
used to evaluate gradients of functions from $\reals^N$ to $\reals$ or
Jacobians of differentiable functions from $\reals^N$ to $\reals^M$,
returning values for a specified input point $x \in \reals^N$; see
\refsection{derivative-definitions} for precise definitions of
gradients and Jacobians and their evaluation at a point.

\subsection{Direct Calculation of Gradients of Programs}

The following complete C++ program calculates derivatives of the
function given in \refeq{normal-log-density} with respect to its mean
($\mu$) and standard deviation ($\sigma$) parameters for a constant
outcome ($y$).  The first block assigns the constant and independent
variables, the second block computes the dependent variable and prints
the value, and the final block computes and prints the gradients.
%
\begin{quote}
\begin{Verbatim}
#include <cmath>
#include <boost/math/constants.hpp>
#include <stan/agrad/rev.hpp>

int main() { 
  double y = 1.3;
  stan::agrad::var mu = 0.5, sigma = 1.2;

  stan::agrad::var lp = 0;
  lp -= pow(2 * boost::math::constants::pi<double>(), -0.5);
  lp -= log(sigma);
  lp -= 0.5 * pow((y - mu) / sigma), 2);
  std::cout << "f(mu,sigma)= " << lp.val() << std::endl;

  lp.grad();
  std::cout << " d.f / d.mu = " << mu.adj()
            << " d.f / d.sigma = " << sigma.adj() << std::endl;
}
\end{Verbatim}
\end{quote}
%
Constants like \code{y} are assigned to type \code{double} variables
and independent variables like \code{mu} and \code{sigma} are assigned
to type \code{var}.  The result \code{lp} is assigned type \code{var}
and calculated using ordinary C++ operations involving operators
(e.g., \code{*}, \code{/}), compound assignments (e.g., \code{-=}),
and library functions (e.g., \code{pow}, \code{log}, \code{pi}).  The
value is available through the member function \code{val()} as soon as
operations have been applied.  The call to the member function
\code{grad()} propagates derivatives from the dependent variable
\code{lp} down through the expression graph to the independent
variables.  The derivatives of the dependent variable with respect to
the independent variables may then be extracted from the independent
variables with the member function \code{adj()}.

The gradient evaluated at the input can also be extracted as a
standard vector using the member function \code{grad()} of
\code{agrad::var}.
%
\begin{quote}
\begin{Verbatim}
#include <vector>
...
  std::vector<stan::agrad::var> theta;
  theta.push_back(mu);   theta.push_back(sigma);
  std::vector<double> g;
  lp.grad(theta,g);
  std::cout << " d.f / d.mu = " << g[0]
            << " d.f / d.sigma = " << g[1] << std::endl;
\end{Verbatim}
\end{quote}
%
The ellision (\code{...}) represents all of the code from the previous
program up to but not including the final block.  The standard vector
\code{theta} holds the dependent variables and the standard vector
\code{g} is used to hold the result.  The function \code{grad()}
function is called on the dependent variable to propagate the
derivatives and fill \code{g} with the gradient.

\subsection{Coding Template Functions for Automatic Differentiation}

The previous example was implemented directly in the main function
block using primitive operations.  The \code{stan::agrad} library
implements all of the built-in C++ operators, (compound) assignment
operators, and library functions in a way to be described later.  From
a user perspective, a function can be automatically differentiated
with respect to some input parameters if the arguments to be
differentiated can all be instantiated to \code{agrad::var} types.
For the most flexiblity, functions should be separately templated in
all of their arguments so as to support any combination of primitive
(e.g., \code{double}, \code{int}) and autodiff (\code{agrad::var})
instantiations.  

For example, the following templated C++ function computes the log
normal density as defined in \refeq{normal-log-density}.
%%
\begin{quote}
\begin{Verbatim}
#include <boost/math/tools/promotion.hpp>

template <typename T1, typename T2, typename T3>
inline
typename boost::math::tools::promote_args<T1,T2,T3>::type
normal_log(const T1& y1, const T2& y2, const T3& y3) {
  using std::pow;  using std::log;  
  return -0.5 * pow((y - mu) / sigma, 2)
      - log(sigma)
      - 1 / sqrt(2 * pi<double>());
}
\end{Verbatim}
\end{quote}
%

\subsubsection{Argument-Dependent Lookup for Function Resolution}

In order to allow built-in functions such as \code{log()} and
\code{pow()} to be instantiated with arguments of type
\code{agrad::var} and primitive C++ types, the primitive version of
the function is brought in with a \code{using} statement, as in
\code{using~std::pow;}.  This brings the version of \code{pow()} that
applies to \code{double} arguments into scope.  The definition of
\code{pow()} for autodiff variables \code{stan::agrad} is brought in
through argument-dependent lookup
\cite[Section~3.4]{cpp-standard:2003}, which brings the namespace of
any argument variables into scope for the purposes of resolving
function applications.


\subsubsection{Traits Metaprogram for Computing Return Types}

Boost's traits-based metaprogram \code{promote\_args}
\citep{Boost:2011} is designed to calculate return types for highly
templated functions like \code{normal\_log()}.

In general, for an automatically differentiated function, the return
type should be \code{double} if all the input arguments are primitive
integers or double-precision floating point values, and
\code{agrad::var} if any of the arguments is of type
\code{agrad::var}.

Given a sequence of types $\code{T1}, \ldots, \code{TN}$, the template
structure \code{promote\_args<T1,...,TN> } defines a typedef named
\code{type}, which is defined to be \code{double} if all the inputs
are \code{int} or \code{double}, and \code{agrad::var} if any of the
input types is \code{agrad::var}.  The keyword \code{typename} is
required to let the C++ parser know that the member variable is a
typedef rather than a regular value.  The Boost promotion mechanism is
used throughout the \code{stan::agrad} library to define both return
types and types of variables for intermediate results.  For instance,
\code{y~-~mu} would be assigned to an intermediate variable of type
\code{promote\_args<T1,T2>::type}.

The fully templated definition allows the template parameters
\code{T1}, \code{T2}, or \code{T3} to be instantiated as independent
variables\, \mbox{\rm (\code{agrad::var})} or constants\, \mbox{\rm
  (\code{double}, \code{int})}.  For example, it can be used in place
of the direct definitions in the previous programs as
%
\begin{quote}
\begin{Verbatim}
...
double y = 1.3;
stan::agrad::var mu = 0.5, sigma = 1.2;

stan::agrad::var lp = normal_log(y,mu,sigma);
...
\end{Verbatim}
\end{quote}




\subsection{Calculating the Derivatives of Functors with Functionals}

The \code{stan::agrad} library provides a fully abstracted approach to
automatic differentiation that uses a C++ functor to represent a
function to be differentiated and a functional for the gradient
operator.  

\subsubsection{Eigen C++ Library for Matrices and Linear Algebra} 

Here and throughout \code{stan::agrad}, the Eigen C++ library for
matrix operations and linear algebra is used \cite{Eigen:2013}.

The type \code{Matrix<T,R,C>} is the Eigen type for matrices
containing elements of type \code{T}, with row and column type
\code{C}.  The \code{stan::agrad} library uses three possible
instantiations, \code{Matrix<T,Dynamic,1>} for (column) vectors,
\code{Matrix<T,1,Dynamic>} for row vectors, and
\code{Matrix<T,Dynamic,Dynamic>} for matrices.  These three instances
are all specialized with their own operators in \code{Eigen}.  Like
the standard template library's \code{std::vector} class, these all
allocate memory for elements dynamically on the C++ heap.

\subsubsection{Definining Functors in C++}

A functor in C++ is a function that defines \code{operator()} and can
hence behaves syntactically like a function.  For example, the normal log
likelihood may be defined directly as a functor as follows.
%
\begin{quote}
\begin{Verbatim}
#include <stan/agrad/autodiff.hpp>

using Eigen::Matrix;  using Eigen::Dynamic;

struct normal_ll {
  const Matrix<double,Dynamic,1> y_;

  normal_ll(const std::vector<double>& y) : y_(y) { }

  template <typename T>
  T operator()(const Matrix<T,Dynamic,1>& theta) {
    T mu = theta[0];   T sigma = theta[1];
    T lp = 0;
    for (size_t n = 0; n < y_.size(); ++n)
      lp += normal_log(y_[n],mu,sigma);
    return lp;
  }
};
\end{Verbatim}
\end{quote}
%
The variable \code{y\_} is used to store the data vector in the
structure.  The \code{operator()} defines a function whose argument
argument type is the Eigen type for vectors with elements of type
\code{T}; the result is also defined to be of type \code{T}. 


\subsubsection{Functionals in C++}

A functional in C++ is a function that applies to a functor.  For
functional autodiff in \code{stan::agrad}, the functor must define an
\code{operator()} that can be applied to an Eigen vector containing
elements of type \code{agrad::var}.  In the running example, the
elements of the vector argument are declared to have the type of the
template parameter \code{T}, which can be instantiated to
\code{agrad::var} for differentiation and to \code{double} for
testing.

The following code calculate the gradient of the functor at a
specified input point.
%
\begin{quote}
\begin{Verbatim}
Matrix<double,Dynamic,1> y(3);
y << 1.3, 2.7, -1.9;
normal_ll f(y);

Matrix<double,Dynamic,1> theta(2);
theta << 1.3, 2.9;

Matrix<double,Dynamic,1> grad_fx;
double fx;
stan::agrad::gradient(f, x, fx, grad_fx);
\end{Verbatim}
\end{quote}
%
The log likelihood function is instantiated as \code{f} with a vector
of data.  The argument \code{theta} is instantiated with the parameter
values.  Then a scalar \code{fx} is defined to hold the function value
and an Eigen vector \code{grad\_fx} is defined to hold the gradient.
The function \code{gradient} is then called to calculate the value and
the gradient from the function \code{f} and input \code{x}.







\subsection{Calculating Jacobians}

Mathematically, Jacobians are simply repeated gradient calculations,
one for each output of a multivariate function; see
\refsection{derivative-definitions} for an exact definition.
Computationally, they work the same way, requiring a single forward
pass to construct all of the outputs, then one reverse pass for each
output dimension.

\subsubsection{Direct Jacobian Calculation}

Suppose that \code{f} is a functor that accepts an dimensional Eigen
$N$-vector of autodiff variables as input (typically through
templating) and produces an Eigen $M$-vector as output.
The following code calculates the Jacobian of \code{f} evaluated 
at the input \code{x}.

\begin{quote}
\begin{Verbatim}
Matrix<double,Dynamic,1> x = ...;   // inputs

Matrix<var,Dynamic,1> x_var(x.size());
for (int i = 0; i < x.size(); ++i) x_var(i) = x(i);

Matrix<var,Dynamic,1> f_x_var = f(x_var);

Matrix<double,Dynamic,1> f_x(f_x_var.size());
for (int i = 0; i < f_x.size(); ++i) f_x(i) = f_x_var(i).val();

Matrix<double,Dynamic,Dynamic> J;
for (int i = 0; i < f_x_var.size(); ++i) {
  if (i > 0) stan::agrad::set_zero_all_adjoints();
  fx_var(i).grad();
  for (int j = 0; j < x_var.size(); ++j)
    J(i,j) = x_var(j).adj();
}
\end{Verbatim}
\end{quote}
%
First, the arguments are used to create autodiff variables.  Next, the
function is applied and the result is converted back to a vector of
doubles.  Then for each output dimension, automatic differentiation
calculates the gradient of that dimension and uses it to populate a
row of the Jacobian.  After the first gradient is calculated, all
subsequent gradient calculations begin by setting the adjoints to
zero; this is not required for the frist gradient because the adjoints
are initialized to zero.


\subsubsection{Functional Jacobian Calculation}

Alternatively, the Jacobian functional can be applied directly to the
function \code{f}, as follows.
%
\begin{quote}
\begin{Verbatim}
...
Matrix<double,Dynamic,Dynamic> J;
matrix<double,Dynamic,1> f_x;
stan::agrad::jacobian(f, x, f_x, J);
\end{Verbatim}
\end{quote}



\section{How \code{\bfseries stan::agrad} Works}

This section explains in detail how the data structures and algorithms
of \code{stan::agrad} work.


\subsection{Autodiff Variables}

As seen in the previous section, the workhorse data type of
\code{stan::agrad} is the type \code{stan::agrad::var}.  

\subsubsection{Pointers to Implementations}

The \code{var} class is implemented following the pointer to
implementation (Pimpl) pattern \citep{sutter:98,sutter:01}.  The
implementation class here is of type \code{vari}, and is covered in
the next subsection.  Like the factory pattern, the Pimpl pattern
encapsulates the details of the implementation class(es), which the
API need not expose to clients.  For example, none of the example
programs in the last section involved the \code{vari} type explicitly.
Pimpl classes also encapsulate all the messy pointer semantics,
providing easily managed stack-based classes.

\subsubsection{Resource Allocation is Initialization}

Like many Pimpl classes, \code{var} manages memory using a mechanism
in which resource allocation is initialization (RAII) pattern
\citep{stroustrup:94}.  With RAII, instances are managed on the stack
and passed to functions just like primitive types.  Memory management
is handled behind the scenes of the client-facing application
programmer interface (API).

Other classes implemented using Pimpl and RAII include
\code{std::vector} and \code{Eigen::Matrix}, both of which allocate
memory directly on the C++ heap and deallocate it when their variables
go out of scope and their destructors are called.  As explained below,
memory is managed slightly differently for \code{agrad::var}
instances.

\subsubsection{The \code{var} Class}

The core of the \code{stan::agrad::var} class is just implemented as a
pointer to implementations.
%
\begin{quote}
\begin{Verbatim}
class var {
private:
  vari* vi_;
public:
  var() : vi_(static_cast<vari*>(0U)) { }
  var(double v) : vi_(new vari(v)) { }

  double val() const { return vi_->val_; }
  double adj() const { return vi_->adj_; }  
};
\end{Verbatim}
\end{quote}
%
The defualt constructor \code{var()} produces a null pointer.  When
instantiated with a \code{double} value, a new \code{vari} instance is
constructed for the value.  The memory management for \code{vari} is
handled through a specialization of \code{operator}~\code{new} as
described below.

The \code{var} class additionally defines a copy constructor,
constructors for the other primitive types, and the full set of
assignment and compound assignment operators.  The destructor is
implicit because there is nothing to do to clean up instances; all of
the work is done with the memory management for \code{vari}. The class
is defined in the \code{stan::agrad} namespace, but the namespace
declarations are not shown to save
space.

\subsubsection{The \code{chainable} Base Class}

The definition of the base variable implementation class, \code{vari},
is shown in \reffigure{vari-definition}.  The base class holds the
variable's value and adjoint.  The design is object-oriented, with the
intention being that subclasses will extend \code{vari} and implement the
virtual method \code{chain()} in a function-specific way to propagate
derivatives with the chain rule.  The class \code{vari} itself extends
the base class \code{chainable}, which holds object which participate
in the chain rule but do not simply hold values and adjoints (e.g.,
some matrix operations).

\begin{quote}
\begin{Verbatim}
struct chainable {
  chainable() { }
  virtual ~chainable() { }

  virtual void chain() { }
  virtual void init_dependent() { }
  virtual void set_zero_adjoint() { }

  static inline void* operator new(size_t nbytes) {
    return ChainableStack::memalloc_.alloc(nbytes);
  }
};
\end{Verbatim}
\end{quote}
%
The core of the \code{agrad::chainable} base class.  The virtual
method \code{chain()} implements derivative propagation for extensions
of \code{chainable}.  There is a static specialization of
\code{operator}~\code{new}, which handles custom arena-based memory
management.  The initialization and set-zero are virtual methods used
to initialize the adjoints during the derivative propagation (reverse)
pass.

\subsubsection{The \code{vari} Class}

\begin{quote}
\begin{Verbatim}
class vari : public chainable {
public:
  const double val_;
  double adj_;

  vari(double v) : val_(v), adj_(0) { 
    ChainableStack::var_stack_.push_back(this);
  }

  virtual ~vari() { }

  virtual void init_dependent() { adj_ = 1; }
  virtual void set_zero_adjoint() { adj_ = 0; }
};
\end{Verbatim}
\end{quote}
%
The core of the \code{agrad::vari} class, which extends the base class
\code{agrad::chainable} and holds the actual value and adjoint for an
autodiff variable instantiation.  namespace.  The adjoint manipulation
methods are defined to set the adjoint to 1 when initializing the
dependent variable's adjoint and 0 for the set-to-zero
operation. There is a further constructor that manages memory slightly
differently that will be discussed
later.


\subsection{Memory Management}

Instances of \code{vari} and its extensions are allocated using the
specialized static \code{operator}~\code{new} definition.  The specialization
references the \code{alloc(size\_t)} method of the variable
\code{ChainableStack::memalloc\_}, which provides a custom arena-based
memory management for variable implementations.

\begin{quote}
\begin{Verbatim}
template<typename T, typename AllocT>
struct AutodiffStackStorage {
  static std::vector<T*> var_stack_;
  static std::vector<AllocT*> var_alloc_stack_;
  static memory::stack_alloc memalloc_;
};

struct chainable_alloc {
  chainable_alloc() {
    ChainableStack::var_alloc_stack_.push_back(this);
  }
  virtual ~chainable_alloc() { };
};

typedef AutodiffStackStorage<chainable,chainable_alloc>
  ChainableStack;
\end{Verbatim}
\end{quote}
%
The core of the \code{agrad::ChainableStack} template class
definition, which holds the memory arena for reverse-mode autodiff.
The key components are \code{var\_stack\_}, which holds all of the
usual autodiff variables and is traversed to propagate derivatives,
\code{var\_alloc\_stack}, which holds objects allocated for autodiff
that need to have their \code{delete()} methods called, and
\code{memalloc\_}, which is the byte-level memory
manager.














\clearpage
\appendix

\section{Derivatives}\label{derivative-definitions.section}

\subsection{Gradients}

Suppose $f : \reals^N \rightarrow \reals$ is a continuously
differentiatble $N$-ary function.  The gradient operator
$\nabla:(\reals^N \rightarrow \reals)\rightarrow (\reals^N \rightarrow
\reals^N)$ is defined so that $(\nabla f):\reals^N \rightarrow
\reals^N$ is a function mapping a point $x \in \reals^N$ to the
derivative vector of $f$ evaluated at $x$, namely
%
\[
(\nabla f)(x) 
= 
\left[
  \left. 
    \frac{\partial}{\partial u_1} f(u_1,x_2,\ldots,x_N)
  \right|_{u_1 = x_1}
  \ \ \cdots \ \ 
  \left. 
    \frac{\partial}{\partial u_N}  f(x_1,\ldots,x_{N-1},u_N)
   \right|_{u_N = x_N}
\right]
\]
%
Here new variables $u_n$ are introduced as bound variables to
avoid any confusion arising from using $x_n$ both as a component of
the evaluation point $x$ and as a bound variable for differentiation.

\section{Jacobians}

The Jacobian of a function $g : \reals^N \rightarrow \reals^M$ is just
the matrix formed by stacking the gradients of $g$ projected down to
each of the $M$ result dimensions,
%
\[
J(g)(x) = 
\left[
\begin{array}{c}
(\nabla g_1)(x)
\\[2pt]
\vdots
\\[2pt]
(\nabla g_M)(x)
\end{array}
\right]
\]
%
where $g_m(x) = g(x)_m$, the $m$-th component of the result, by definition.

\clearpage
\nocite{Hogan:2014}
\nocite{Neal:2003}

\bibliographystyle{apalike}
\bibliography{../../bibtex/all}


\end{document}